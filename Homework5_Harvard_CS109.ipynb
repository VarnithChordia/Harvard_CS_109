{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ggplot import *\n",
    "import datetime as dt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Vino Veritas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can a winemaker predict how a wine will be received based on the chemical properties of the wine? Are there chemical indicators that correlate more strongly with the perceived \"quality\" of a wine?\n",
    "\n",
    "In this problem we'll examine the wine quality dataset hosted on the UCI website. This data records 11 chemical properties (such as the concentrations of sugar, citric acid, alcohol, pH etc.) of thousands of red and white wines from northern Portugal, as well as the quality of the wines, recorded on a scale from 1 to 10. In this problem, we will only look at the data for red wine.\n",
    "\n",
    "### Problem 1: Data Collection\n",
    "Import only the data for red wine from the dataset repository. Build a pandas dataframe from the csv file and print the head. You might have to change the default delimiter used by the read_csv function in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.069</td>\n",
       "      <td>15.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.99640</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.065</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.99460</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.47</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.073</td>\n",
       "      <td>9.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.57</td>\n",
       "      <td>9.5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.071</td>\n",
       "      <td>17.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.80</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.097</td>\n",
       "      <td>15.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.99590</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.071</td>\n",
       "      <td>17.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.80</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.6</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.089</td>\n",
       "      <td>16.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.99430</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>9.9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.114</td>\n",
       "      <td>9.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.99740</td>\n",
       "      <td>3.26</td>\n",
       "      <td>1.56</td>\n",
       "      <td>9.1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.9</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.18</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.176</td>\n",
       "      <td>52.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.99860</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.88</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8.9</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.19</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.170</td>\n",
       "      <td>51.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.99860</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.93</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.5</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.092</td>\n",
       "      <td>35.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.99690</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.75</td>\n",
       "      <td>10.5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.368</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.11</td>\n",
       "      <td>1.28</td>\n",
       "      <td>9.3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.086</td>\n",
       "      <td>6.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.99740</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.341</td>\n",
       "      <td>17.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.99690</td>\n",
       "      <td>3.04</td>\n",
       "      <td>1.08</td>\n",
       "      <td>9.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.9</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.077</td>\n",
       "      <td>29.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.53</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.082</td>\n",
       "      <td>23.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.99820</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.106</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.99660</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.91</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.5</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.084</td>\n",
       "      <td>9.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.53</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.085</td>\n",
       "      <td>21.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0.63</td>\n",
       "      <td>9.7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.080</td>\n",
       "      <td>11.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.99550</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.080</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.99620</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.59</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.106</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.99660</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.91</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.080</td>\n",
       "      <td>14.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.99720</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.55</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.082</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.99640</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.59</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.056</td>\n",
       "      <td>15.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99396</td>\n",
       "      <td>3.48</td>\n",
       "      <td>0.57</td>\n",
       "      <td>11.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.230</td>\n",
       "      <td>19.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.99340</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.93</td>\n",
       "      <td>12.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.038</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.99514</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.65</td>\n",
       "      <td>11.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.069</td>\n",
       "      <td>35.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.99632</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.075</td>\n",
       "      <td>15.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.99467</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.67</td>\n",
       "      <td>12.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>5.6</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.78</td>\n",
       "      <td>13.9</td>\n",
       "      <td>0.074</td>\n",
       "      <td>23.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99677</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.48</td>\n",
       "      <td>10.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.40</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.060</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.99474</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.64</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.081</td>\n",
       "      <td>16.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.99588</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.78</td>\n",
       "      <td>10.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.076</td>\n",
       "      <td>13.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.99622</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.118</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.99540</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.67</td>\n",
       "      <td>11.3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.053</td>\n",
       "      <td>24.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.99402</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11.3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.068</td>\n",
       "      <td>9.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.99470</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.053</td>\n",
       "      <td>24.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.99402</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11.3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.053</td>\n",
       "      <td>13.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.99362</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.074</td>\n",
       "      <td>32.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.99578</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.62</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.44</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.061</td>\n",
       "      <td>24.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99484</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.80</td>\n",
       "      <td>11.6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.44</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.066</td>\n",
       "      <td>22.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.99494</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.84</td>\n",
       "      <td>11.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.41</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.065</td>\n",
       "      <td>34.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99492</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.85</td>\n",
       "      <td>11.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>5.8</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.066</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.99483</td>\n",
       "      <td>3.55</td>\n",
       "      <td>0.66</td>\n",
       "      <td>10.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.068</td>\n",
       "      <td>34.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.99414</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.78</td>\n",
       "      <td>12.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.20</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.073</td>\n",
       "      <td>29.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.99770</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.077</td>\n",
       "      <td>26.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.99314</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.82</td>\n",
       "      <td>11.6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>5.4</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.089</td>\n",
       "      <td>16.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.99402</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.56</td>\n",
       "      <td>11.6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.068</td>\n",
       "      <td>28.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.99651</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.82</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "5               7.4             0.660         0.00             1.8      0.075   \n",
       "6               7.9             0.600         0.06             1.6      0.069   \n",
       "7               7.3             0.650         0.00             1.2      0.065   \n",
       "8               7.8             0.580         0.02             2.0      0.073   \n",
       "9               7.5             0.500         0.36             6.1      0.071   \n",
       "10              6.7             0.580         0.08             1.8      0.097   \n",
       "11              7.5             0.500         0.36             6.1      0.071   \n",
       "12              5.6             0.615         0.00             1.6      0.089   \n",
       "13              7.8             0.610         0.29             1.6      0.114   \n",
       "14              8.9             0.620         0.18             3.8      0.176   \n",
       "15              8.9             0.620         0.19             3.9      0.170   \n",
       "16              8.5             0.280         0.56             1.8      0.092   \n",
       "17              8.1             0.560         0.28             1.7      0.368   \n",
       "18              7.4             0.590         0.08             4.4      0.086   \n",
       "19              7.9             0.320         0.51             1.8      0.341   \n",
       "20              8.9             0.220         0.48             1.8      0.077   \n",
       "21              7.6             0.390         0.31             2.3      0.082   \n",
       "22              7.9             0.430         0.21             1.6      0.106   \n",
       "23              8.5             0.490         0.11             2.3      0.084   \n",
       "24              6.9             0.400         0.14             2.4      0.085   \n",
       "25              6.3             0.390         0.16             1.4      0.080   \n",
       "26              7.6             0.410         0.24             1.8      0.080   \n",
       "27              7.9             0.430         0.21             1.6      0.106   \n",
       "28              7.1             0.710         0.00             1.9      0.080   \n",
       "29              7.8             0.645         0.00             2.0      0.082   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1569            6.2             0.510         0.14             1.9      0.056   \n",
       "1570            6.4             0.360         0.53             2.2      0.230   \n",
       "1571            6.4             0.380         0.14             2.2      0.038   \n",
       "1572            7.3             0.690         0.32             2.2      0.069   \n",
       "1573            6.0             0.580         0.20             2.4      0.075   \n",
       "1574            5.6             0.310         0.78            13.9      0.074   \n",
       "1575            7.5             0.520         0.40             2.2      0.060   \n",
       "1576            8.0             0.300         0.63             1.6      0.081   \n",
       "1577            6.2             0.700         0.15             5.1      0.076   \n",
       "1578            6.8             0.670         0.15             1.8      0.118   \n",
       "1579            6.2             0.560         0.09             1.7      0.053   \n",
       "1580            7.4             0.350         0.33             2.4      0.068   \n",
       "1581            6.2             0.560         0.09             1.7      0.053   \n",
       "1582            6.1             0.715         0.10             2.6      0.053   \n",
       "1583            6.2             0.460         0.29             2.1      0.074   \n",
       "1584            6.7             0.320         0.44             2.4      0.061   \n",
       "1585            7.2             0.390         0.44             2.6      0.066   \n",
       "1586            7.5             0.310         0.41             2.4      0.065   \n",
       "1587            5.8             0.610         0.11             1.8      0.066   \n",
       "1588            7.2             0.660         0.33             2.5      0.068   \n",
       "1589            6.6             0.725         0.20             7.8      0.073   \n",
       "1590            6.3             0.550         0.15             1.8      0.077   \n",
       "1591            5.4             0.740         0.09             1.7      0.089   \n",
       "1592            6.3             0.510         0.13             2.3      0.076   \n",
       "1593            6.8             0.620         0.08             1.9      0.068   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "5                    13.0                  40.0  0.99780  3.51       0.56   \n",
       "6                    15.0                  59.0  0.99640  3.30       0.46   \n",
       "7                    15.0                  21.0  0.99460  3.39       0.47   \n",
       "8                     9.0                  18.0  0.99680  3.36       0.57   \n",
       "9                    17.0                 102.0  0.99780  3.35       0.80   \n",
       "10                   15.0                  65.0  0.99590  3.28       0.54   \n",
       "11                   17.0                 102.0  0.99780  3.35       0.80   \n",
       "12                   16.0                  59.0  0.99430  3.58       0.52   \n",
       "13                    9.0                  29.0  0.99740  3.26       1.56   \n",
       "14                   52.0                 145.0  0.99860  3.16       0.88   \n",
       "15                   51.0                 148.0  0.99860  3.17       0.93   \n",
       "16                   35.0                 103.0  0.99690  3.30       0.75   \n",
       "17                   16.0                  56.0  0.99680  3.11       1.28   \n",
       "18                    6.0                  29.0  0.99740  3.38       0.50   \n",
       "19                   17.0                  56.0  0.99690  3.04       1.08   \n",
       "20                   29.0                  60.0  0.99680  3.39       0.53   \n",
       "21                   23.0                  71.0  0.99820  3.52       0.65   \n",
       "22                   10.0                  37.0  0.99660  3.17       0.91   \n",
       "23                    9.0                  67.0  0.99680  3.17       0.53   \n",
       "24                   21.0                  40.0  0.99680  3.43       0.63   \n",
       "25                   11.0                  23.0  0.99550  3.34       0.56   \n",
       "26                    4.0                  11.0  0.99620  3.28       0.59   \n",
       "27                   10.0                  37.0  0.99660  3.17       0.91   \n",
       "28                   14.0                  35.0  0.99720  3.47       0.55   \n",
       "29                    8.0                  16.0  0.99640  3.38       0.59   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1569                 15.0                  34.0  0.99396  3.48       0.57   \n",
       "1570                 19.0                  35.0  0.99340  3.37       0.93   \n",
       "1571                 15.0                  25.0  0.99514  3.44       0.65   \n",
       "1572                 35.0                 104.0  0.99632  3.33       0.51   \n",
       "1573                 15.0                  50.0  0.99467  3.58       0.67   \n",
       "1574                 23.0                  92.0  0.99677  3.39       0.48   \n",
       "1575                 12.0                  20.0  0.99474  3.26       0.64   \n",
       "1576                 16.0                  29.0  0.99588  3.30       0.78   \n",
       "1577                 13.0                  27.0  0.99622  3.54       0.60   \n",
       "1578                 13.0                  20.0  0.99540  3.42       0.67   \n",
       "1579                 24.0                  32.0  0.99402  3.54       0.60   \n",
       "1580                  9.0                  26.0  0.99470  3.36       0.60   \n",
       "1581                 24.0                  32.0  0.99402  3.54       0.60   \n",
       "1582                 13.0                  27.0  0.99362  3.57       0.50   \n",
       "1583                 32.0                  98.0  0.99578  3.33       0.62   \n",
       "1584                 24.0                  34.0  0.99484  3.29       0.80   \n",
       "1585                 22.0                  48.0  0.99494  3.30       0.84   \n",
       "1586                 34.0                  60.0  0.99492  3.34       0.85   \n",
       "1587                 18.0                  28.0  0.99483  3.55       0.66   \n",
       "1588                 34.0                 102.0  0.99414  3.27       0.78   \n",
       "1589                 29.0                  79.0  0.99770  3.29       0.54   \n",
       "1590                 26.0                  35.0  0.99314  3.32       0.82   \n",
       "1591                 16.0                  26.0  0.99402  3.67       0.56   \n",
       "1592                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1593                 28.0                  38.0  0.99651  3.42       0.82   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "5         9.4        5  \n",
       "6         9.4        5  \n",
       "7        10.0        7  \n",
       "8         9.5        7  \n",
       "9        10.5        5  \n",
       "10        9.2        5  \n",
       "11       10.5        5  \n",
       "12        9.9        5  \n",
       "13        9.1        5  \n",
       "14        9.2        5  \n",
       "15        9.2        5  \n",
       "16       10.5        7  \n",
       "17        9.3        5  \n",
       "18        9.0        4  \n",
       "19        9.2        6  \n",
       "20        9.4        6  \n",
       "21        9.7        5  \n",
       "22        9.5        5  \n",
       "23        9.4        5  \n",
       "24        9.7        6  \n",
       "25        9.3        5  \n",
       "26        9.5        5  \n",
       "27        9.5        5  \n",
       "28        9.4        5  \n",
       "29        9.8        6  \n",
       "...       ...      ...  \n",
       "1569     11.5        6  \n",
       "1570     12.4        6  \n",
       "1571     11.1        6  \n",
       "1572      9.5        5  \n",
       "1573     12.5        6  \n",
       "1574     10.5        6  \n",
       "1575     11.8        6  \n",
       "1576     10.8        6  \n",
       "1577     11.9        6  \n",
       "1578     11.3        6  \n",
       "1579     11.3        5  \n",
       "1580     11.9        6  \n",
       "1581     11.3        5  \n",
       "1582     11.9        5  \n",
       "1583      9.8        5  \n",
       "1584     11.6        7  \n",
       "1585     11.5        6  \n",
       "1586     11.4        6  \n",
       "1587     10.9        6  \n",
       "1588     12.8        6  \n",
       "1589      9.2        5  \n",
       "1590     11.6        6  \n",
       "1591     11.6        6  \n",
       "1592     11.0        6  \n",
       "1593      9.5        6  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wine=pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv',sep=';')\n",
    "Wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in any machine learning problem, we have the feature data, usually labeled as X, and the target data, labeled Y. Every row in the matrix XX is a datapoint (i.e. a wine) and every column in X is a feature of the data (e.g. pH). For a classification problem, Y is a column vector containing the class of every datapoint.\n",
    "\n",
    "We will use the quality column as our target variable. Save the quality column as a separate numpy array (labeled YY) and remove the quality column from the dataframe.\n",
    "\n",
    "Also, we will simplify the problem to a binary world in which wines are either \"bad\" (score<7score<7) or \"good\" (score≥7)score≥7). Change the Y array accordingly such that it only contains zeros (\"bad\" wines) and ones (\"good\" wines). For example, if originally Y=[1,3,8,4,7]Y=[1,3,8,4,7], the new Y should be [0,0,1,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y=Wine.quality.values\n",
    "del Wine['quality']\n",
    "Y = np.array([1 if y>=7 else 0 for y in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=Wine.as_matrix()\n",
    "Y=list(Y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: \n",
    "Unbalanced Classification Evaluation\n",
    "In this section, we explore a number of different methods to predict the quality of a wine YY based on the recorded features XX. Formulated as a machine learning problem, we wish to predict the target YY as a function of the features XX.\n",
    "\n",
    "Because we have defined YY as a binary variable (encoding bad as 0 and good as 1), this is a classification problem. In class, we have discussed several approaches to classifiction incuding decision trees, random forests, and Support Vector Machines (SVM).\n",
    "\n",
    "For this problem, we will focus on random forests, but we will later in the Problem set invoke these other techniques. Recall from class that the random forest technique works by aggregating the results from a number of randomly perturbed decision trees constructed to explain the data.\n",
    "\n",
    "(a) In class, we saw that for a fixed set of data, a decision tree algorithm will generate a single fixed tree to perform a classification task. Describe how a random forest is built from individual decision trees. What are the sources of randomness in the process that are used to build a diverse set of decision trees?\n",
    "\n",
    "YOUR ANSWER HERE.\n",
    "\n",
    "(b) There are many ways to construct a random forest -- these differences in the method of construction are encoded as tuning parameters. As is often the case when our goal is to construct a good prediction, we can set these tuning parameters to obtain the best projected performance in a prediction task. One of the most important tuning parameters in building a random forest is the number of trees to construct.\n",
    "\n",
    "Here, you should apply the random forest classifier to the wine data and use cross-validation to explore how the score of the classifier changes when varying the number of trees in the forest. Use the random forest classifier built into the scikit-learn library and the cross_val_score function (using the default scoring method) to plot the scores of the random forests as a function of the number of trees in the random forest, ranging from 1 (simple decision tree) to 40. You should use 10-fold cross-validation. Feel free to use the boxplot functionality of the seaborn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.82042237978046018,\n",
       " 0.86366540196882691,\n",
       " 0.84170421696160003,\n",
       " 0.86927477635845152,\n",
       " 0.85865356068596432,\n",
       " 0.86553651998124936,\n",
       " 0.86175905308801126,\n",
       " 0.86488808156568608,\n",
       " 0.86177487401851638,\n",
       " 0.87241584143911888,\n",
       " 0.85990385366615885,\n",
       " 0.86866952810656672,\n",
       " 0.86239992284854883,\n",
       " 0.87365404898628862,\n",
       " 0.8855643775147467,\n",
       " 0.87054841009414419,\n",
       " 0.87490400015625625,\n",
       " 0.8761813205593969,\n",
       " 0.87306816184225955,\n",
       " 0.87239997167858119,\n",
       " 0.87618918219461706,\n",
       " 0.87177885366615882,\n",
       " 0.87243136938942933,\n",
       " 0.87368515371694211,\n",
       " 0.8686537071760615,\n",
       " 0.87116584143911879,\n",
       " 0.87052895132622365,\n",
       " 0.87306025137700694,\n",
       " 0.87677113852103594,\n",
       " 0.87305243857181924,\n",
       " 0.87493141821946163,\n",
       " 0.86551701238329615,\n",
       " 0.87929845892417668,\n",
       " 0.87179860541427401,\n",
       " 0.87115006933864603,\n",
       " 0.87806423102464959,\n",
       " 0.86802502050861352,\n",
       " 0.87493923102464954,\n",
       " 0.87868156470955905,\n",
       " 0.87743923102464938]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList = list(range(1,41))\n",
    "cv_scores = []\n",
    "for k in myList:\n",
    "    Rf = RandomForestClassifier(n_estimators=k)\n",
    "    scores = cross_val_score(Rf, X,Y, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuEAAAHvCAYAAAD6ogF/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xdgzdf/P/DnvVkSkWWFGBFB9rSDlJCWqKL2KKp8jPJF\ntanV0laV0oGqWqUqqPExKkKMGkGWbBqamomQHZk3yb2/P/xyPyLz3twZz8c/7b33fd/vV165N173\n3Nc5RyCRSCQgIiIiIiKVEao7ACIiIiKi1w2LcCIiIiIiFWMRTkRERESkYizCiYiIiIhUjEU4ERER\nEZGKsQgnIiIiIlIxXXUHoA1SUlJkfo6enh6aN2+OtLQ0lJSUKCEq2RkYGKC4uFjdYTA3tWB+qsfc\n1Iz5qR5zUzPmp3rMTc3UkZ/WrVur5DrKxpHw14hQyF93dZibmjE/1WNuasb8VI+5qRnzUz3mpmHg\nb5GIiIiISMVYhBMRERERqRiLcCIiIiIiFWMRTkRERESkYizCiYiIiIhUjEU4EREREZGKsQgnIiIi\nIlIxFuFERERERCrGIpyIiIiISMVYhBMRERERqRiLcCIiIiIiFWMRTkRERESkYizCiYiIiIhUjEU4\nEREREZGKsQgnIiIiIlIxFuFERERERCrGIpyIiIiISMVYhBMRERERqZiuugOoq4KCApw4cQJJSUkw\nMjKCj48PXFxcKh0nkUhw4cIFREdHQyQSwdLSEn5+fmjRogUAIC0tDadOncKTJ09gZGQEX19f2Nvb\nq/rHISIiIqLXmNaMhAcGBkJHRweLFy/GyJEjcerUKTx79qzScQkJCYiKisK0adPg7++Ptm3b4ujR\nowCAsrIy7N+/H507d4a/vz/efvttHD16FOnp6ar+cYiIiIjoNaYVI+EikQi3bt3CnDlzYGBggPbt\n26NLly6IiYnBoEGDKhybnZ2Ndu3awcLCAgDg4uKC69evAwDS09Px/Plz9OrVCwKBADY2Nmjbti1i\nY2MxYMAAAEBubi7y8vIqXb9x48Yyxayrq1vhv5pAR0cHenp66g6DuakF81M95qZmzE/1mJuaMT/V\nY25qpon50RZakbGMjAwIhUI0a9ZMep+lpSXu379f6VgnJyckJCQgPT0d5ubmiI6Ohq2tbY3nf3lE\nPTIyEpcuXarwuLe3N/r37y9X7Obm5nI973XA3NSM+akec1Mz5qd6zE3NmJ/qMTc1Y35kpxVFuEgk\ngoGBQYX7DAwMUFxcXOlYY2NjtGvXDps3b4ZAIICpqSmmTJkCAGjWrBkaN26MkJAQ9OrVC/fu3cP9\n+/fRoUMH6fM9PT3RpUuXStdPS0uTKWZdXV2Ym5sjKysLpaWlMj1XWarLmaoxNzVjfqrH3NSM+ake\nc1Mz5qd6zE3N1JGf5s2bq+Q6yqYVRbi+vn6lF1tRUVGlwhwALl26hOTkZCxcuBDGxsaIjY3Fnj17\nMGfOHOjr62PcuHE4ffo0QkJC0Lp1azg6Olb4CsXExAQmJiYVzpmSkoKSkhK5Yi8tLZX7uYqmq6ur\nMbEAzE1tmJ/qMTc1Y36qx9zUjPmpHnNTM03Kj7bQiiK8adOmEIvFyMjIQNOmTQEAT58+rfKTUGpq\nKpycnGBqagoAcHd3R1BQENLS0mBlZQVLS0tMmzZNevyOHTvg5uammh+EiIiIiAhasjqKvr4+7O3t\ncfHiRYhEIjx48ACJiYlwdXWtdKyVlRUSEhKQl5cHsViMmJgYiMVi6UTN1NRUlJSUQCQSISQkBHl5\neSzCiYiIiEiltGIkHAD8/Pxw/PhxfPvttzA0NJSu/Z2dnY2ffvoJc+fOhZmZGby8vJCfn4+tW7dC\nJBLBwsICY8aMgaGhIQAgNjYWN2/eRFlZGdq3b4/JkydzRi8RERERqZTWVJ9GRkYYP358pfvNzMyw\nbNky6W09PT34+fnBz8+vyvP4+vrC19dXaXESEREREdVGK9pRiIiIiIgaEhbhREREREQqxiKciIiI\niEjFWIQTEREREakYi3AiIiIiIhVjEU5EREREpGIswomIiIiIVIxFOBERERGRirEIJyIiIiJSMRbh\nREREREQqxiKciIiIiEjFWIQTEREREakYi3AiIiIiIhVjEU5EREREpGIswolIK2RlZWHevHno168f\ntm3bpu5wiIiI6kVX3QEQEdXFRx99hDNnzgAArly5gmbNmuHtt99Wc1RERETy4Ug4EWmF+Pj4Gm8T\nERFpExbhRKQVvLy8arxNRESkTViEE5FW+Oabb9ClSxcAwOeff45+/fqpOSIiIiL5sSe8DgwMDCAU\nyvZ5RSAQoKCgAHp6etDV1Yw0C4VCGBoaqjsM5qYWzE/VGjVqhNzcXPTu3RvNmjVTezzlNCE35fja\nqR5zUzPmp3rMTc00MT/agtmqg+LiYpmfo6enBzMzM+Tn56OkpEQJUcnO0NAQhYWF6g6DuakF81O1\nR48eoaysDCNGjEBsbCxGjhyp1njKaUJuyvG1Uz3mpmbMT/WYm5qpIz/m5uYquY6ysR2FiLRCREQE\nunXrBldXV9y6dUvd4RAREdULi3Ai0grh4eHo3r07XF1dkZCQAIlEou6QiIiI5MYinIi0QnkR3qJF\nC+jp6SElJUXdIREREcmNRTgRabznz5/j/v37cHZ2BgA4Ojri9u3bao6KiIhIfizCiUjjRUVFwdnZ\nGfr6+gAABwcH9oUTEZFWYxFORBovPDwc3bp1k97mSDgREWk7FuFEpPEiIiLg6ekpvc2RcCIi0nYs\nwolIo5WVlSEqKgpdu3aV3tepUyc8fvxYY9bJJSIikhWLcCLSaLdv30bLli1hYWEhvU9fXx8dOnTA\nnTt31BgZERGR/FiEE5FGi4iIqDAKXs7BwYF94UREpLVYhBORRivfKfNV7AsnIiJtxiKciDRaeHh4\ntSPhLMKJiEhbsQgnIo315MkT5Ofno2PHjpUeK29H4fb1RESkjViEE5HGKu8HFwgElR5r1qwZ9PT0\n8OTJEzVERkREVD8swolIY726Sc+r2JJCRETaikU4EWmsyMjIKvvBy9nb23OFFCIi0koswolIIxUW\nFiIxMREuLi7VHsORcCIi0lYswolII0VFRcHe3h6GhobVHsORcCIi0lYswolII1W3Sc/LbG1t8ejR\nI25fT0REWodFOBFppNomZQLcvp6IiLQXi3Ai0jhisRg3b96sdSQc4Pb1RESknXTVHUBdFRQU4MSJ\nE0hKSoKRkRF8fHyqnLAlkUhw4cIFREdHQyQSwdLSEn5+fmjRogUAICsrC6dOncLjx4+ho6MDBwcH\nvPXWW9DR0VH1j0RE1fjnn39gamoqfd/WhJMziYhIG2nNSHhgYCB0dHSwePFijBw5EqdOncKzZ88q\nHZeQkICoqChMmzYN/v7+aNu2LY4ePSp9/NSpU2jcuDE++ugjzJo1Cw8ePEB4eLgqfxQiqkV1W9VX\nhUU4ERFpI60owkUiEW7duoX+/fvDwMAA7du3R5cuXRATE1Pp2OzsbLRr1w4WFhYQCoVwcXFBWlpa\nhccdHR2hp6eHJk2awNbWtsLjRKR+dekHL1e+Qgq3ryciIm2iFe0oGRkZEAqFaNasmfQ+S0tL3L9/\nv9KxTk5OSEhIQHp6OszNzREdHQ1bW1vp4z179kRCQgKsra1RVFSEu3fvYsCAAdLHc3NzkZeXV+Gc\nIpEIjRs3lilmXV3dCv/VBDo6OtDT01N3GMxNLZifF5v0zJ07t9I1q8pN69atoaenh/T0dLRu3Vpl\nMZbja6dmmpIf5qZmzE/1mJuaaWJ+tIVWZEwkEsHAwKDCfQYGBiguLq50rLGxMdq1a4fNmzdDIBDA\n1NQUU6ZMkT7evn17REZGYs2aNZBIJHB1dYWdnZ308cjISFy6dKnCOb29vdG/f3+5Yjc3N5frea8D\n5qZmr2t+nj17hszMTPTt2xdCYdVf1r2aG3d3dzx+/Biurq6qCFHjva6vnbpgbmrG/FSPuakZ8yM7\nrSjC9fX1KxXcRUVFlQpzALh06RKSk5OxcOFCGBsbIzY2Fnv27MGcOXOgq6uL33//HZ6enpg+fTpE\nIhGOHz+O4OBg+Pr6AgA8PT3RpUuXCucUiUQyt6zo6urC3NwcWVlZKC0tlfEnVo7qPrioGnNTs9c9\nP6dPn4aHhwcyMjIqPVZdbmxtbXH9+nV0795dJTG+jK+dmmlKfpibmmlLfjZs2IBdu3bBwsICP/74\nIzw8PJQeh7bkRl3UkZ/mzZur5DrKphVFeNOmTSEWi5GRkYGmTZsCAJ4+fVrlLyE1NRVOTk4wNTUF\n8GKELCgoCGlpaTAzM0NOTg66d+8OXV1d6Orqws3NDRcuXJAW4SYmJjAxMalwzpSUFJSUlMgVe2lp\nqdzPVTRdXV2NiQVgbmrzuuYnNDQUHh4eNV7v1dzY2dnh3LlzaskXXzs107T8MDc10+T8XL58GWvX\nrgUApKWlYerUqYiMjFRZPJqcG02gSfnRFloxMVNfXx/29va4ePEiRCIRHjx4gMTExCq/erayskJC\nQgLy8vIgFosRExMDsVgMCwsLNG7cGGZmZoiIiEBZWRkKCwsRExODli1bquGnIqKqyDIpsxy3rydq\n+FJSUircfvbsmcaMTBPJQytGwgHAz88Px48fx7fffgtDQ0Pp2t/Z2dn46aefMHfuXJiZmcHLywv5\n+fnYunUrRCIRLCwsMGbMGBgaGgIAxo4di6CgIFy9ehUCgQAdOnTAW2+9peafjoiAF21mCQkJcHd3\nl+l5L29fX/5eJ6KGpV+/ftK2BwAYOnQoJwMqyIEDBxAcHAwbGxssWrSIf0dVRGtevUZGRhg/fnyl\n+83MzLBs2TLpbT09Pfj5+cHPz6/K87Rq1QrTpk1TWpxEJL+4uDjY2trKvBpR+fb1d+/erXITLyLS\nfq1bt8a2bdswbdo0CIVCLFiwQN0hNQgnT57ERx99JL2dmZmJDRs2qDGi14dWtKMQ0eshIiJC5laU\ncty0h6jhS09Ph5eXF8aNG4djx46pO5wGISIiosLtq1evqimS1w+LcCLSGLLslPkqFuFEDV9cXByc\nnZ3x7rvv4ujRoxCLxeoOqVr//vsvfvnlF5w8eVLdodTo1fa/jIwMjBkzBhcvXuQmaErGIpyINIJE\nIkFERASLcCKqVlxcHFxcXODo6AhjY2OEhYWpO6QqJSUlYciQIfjiiy8wa9YsfPHFF+oOqVoZGRmw\nsrLCG2+8gQ8++AAREREYM2YMVq9ejYEDB+KPP/6ASCRSd5gNEotwItII9+7dg4GBAaysrOR6Prev\nJ2rYJBKJdCRcIBBg1KhROHLkiLrDqlJQUBCeP38uvf3HH3+oMZrqxcfH44cffsDBgwexb98+rFq1\nCmZmZhg1ahSCg4Px2Wef4ejRo+jVqxe2bNmCnJwcdYfcoLAIJyKNUJ9WFODF5g16enp48uSJAqMi\nIk3x6NEjNGrUCC1atAAADB8+HIGBgSgsLFRzZJW9uvSxpaWlmiKpXkFBAebMmYNVq1ahQ4cOlR4X\nCATw9vbGgQMHsGfPHty+fRu9e/fGqlWrkJycLD0uKSkJ9+7dU2XoDQaLcCLSCJGRkXJPyizH9cKJ\nGq7yUfByrVq1grOzM4KDg9UYVdXeffdduLi4QCAQwMjICBs3blR3SJUsX74cHh4eGDlyZK3HOjk5\nYdOmTTh79iwAwNfXF/PmzcO0adPQq1cv2NjYYM2aNcoOucFhEU5EGkGeTXpexb5wooYrNja2QhEO\nvCh2NbElRSwWIy0tDUeOHIFAIICNjY26Q6rg2LFjCAsLw+rVq2V6npWVFT7//HNcu3YNJiYm0qIc\nAL7//nt+EykjFuFEpHZZWVlITk6Gvb19vc5jb2/PIpyogXp1JBwAhgwZgrCwMGRkZKgpqqpdvHgR\nlpaW6NGjB+zs7DRqAumDBw+wYsUK/PzzzzLvyVDO1NQUI0aMqHS/Jq9Wo4lYhBOR2t28eRNubm71\n3v3OwcGB7Sj02vr9998xY8YMfPfddygpKVF3OAolkUgQGxtbaTOuxo0bY+DAgTh+/LiaIqtaQEAA\nJkyYAODFTp9XrlxRc0QvlJSUYO7cuZg/f36lDzSy8vT0xNChQ6W3Z8yYIffE+tcVi3AiUjtFtKIA\nFbevJ3qd/PHHH/D390dgYCA2bNggc5uBpktJSYGenl6VExw1rSUlNTUVN27cwDvvvAMA8Pb2xqVL\nl9Qc1QvffvstzM3N8cEHH9T7XAKBAFu3bkVwcDBiY2Mb3GtOFViEE5Ha1Wd98Je9vH090evk1XaH\n8PBwNUWiHLGxsXBycqrysT59+uDJkyf4559/VBxV1Q4ePIihQ4dKWz3c3Nzw6NEjpKenqzWuy5cv\n48iRI/jhhx8gEAgUck6BQABXV9d6j6q/rliEE5FalZSUICYmBp6engo5nyZMziwqKsLGjRuxfPly\nREZGqjWW2mRlZeHLL7/Exx9/jOjoaHWHQ3J6tQiq7/wKTVNVK0o5XV1dDB8+HIcPH1ZxVJWJxWIc\nOHBA2ooCAHp6eujVq5daW1LS0tKwYMEC/PDDD2jatKna4qCKWIQTkVolJCSgffv2MDExUcj5NKEI\nnzdvHtauXYtff/0Vo0eP1ug+9cmTJ2Pr1q0ICAjA6NGj8eDBA3WHRHLIzs5G+/bt0bdvXzg4OCAn\nJ6dBbVwVHx9fbREOQGO2sb969SqMjY3h6upa4f5+/fqprSVFLBZj4cKFGD16NPr27auWGKhqLMKJ\nSK3Cw8MVNgoOaEYRfu7cOen/FxcXIyQkRI3RVC8/Px9RUVHS2wUFBbh586YaIyJ5hIWF4ddff8Xh\nw4dx4MABnDx5Evfu3UNAQIC6Q1OI8kmZ1bWjAICjoyNMTEwQGhqqwsgq27dvHyZOnFip3aN8cqY6\nPhht27YNOTk5WLx4scqvTTVjEU5EahUREaGQSZnl1Ll9fXx8PObMmYOysrIK93fs2FHlsdRF48aN\n0apVK+ltoVCIzp07qzEiklVWVhY+/PBDfPvtt2jdujUAoFGjRvj555/xzTffIDExUc0R1t+TJ08g\nkUikP1911D1BMyMjA5cvX65y6b4OHTpAT08Pd+7cUWlMMTEx2LJlC7Zs2QI9PT2VXptqxyKciNRG\nIpEovAhX9fb1EokE165dw8SJEzFlyhS4uLjg7NmzGDBgAOzt7dG+fXtcvnxZJbHIqrCwEI0aNYK9\nvT3s7OxgamoKa2trdYdFdSSRSLB48WIMGTIEgwYNqvBYp06dsHz5csyePVvrVwsqb0WpbTLh8OHD\ncfr0abX9vIcOHYKvry9MTU0rPSYQCNCvXz+V/i3Iy8vDnDlz8NVXX6Ft27Yquy7VHYtwIlKbx48f\no6ysDO3atVPoeVWxfb1YLEZQUBB8fX3h7++PoUOH4tq1a5g1axbs7Oywd+9enDt3DoGBgbh48SJ2\n7typ1Hjk8fXXX8PR0RHBwcE4f/48fHx8sGHDBnWHRXW0e/dupKSkYMmSJVU+PmbMGDg4OODzzz9X\ncWSKVVsrSrlWrVrBxcVFLdvYSyQSBAQEYOLEidUeo+oifMmSJejduzeGDRumsmuSbFiEE5HalI+C\nK2q5rHLK7AsXiUQ4ePAg+vfvj40bN2LevHn466+/MH78eBgYGFQ63szMDHv37sWWLVtw+vRppcQk\nj+DgYJw5cwZr166V5n/FihU4fPgwEhIS1Bwd1SY+Ph7fffcdtmzZUuXrDngx+rpmzRqEhITg5MmT\nSolDIpFAJBIp5dzlaloZ5VXvvvuuWlZJCQ0NhVAorPFbPS8vL4SFhaG4uFhpcZSVlSE7OxuHDh1C\nXFwcvvjiC6Vdi+qPRTgRqU14eLhC1gd/laJGwo8cOQIvLy/0798fwcHB2LZtG3r37o1jx47hq6++\nwqlTpzBs2DDo6OjUeJ62bdvi119/xSeffKIRSxY+ffoUH3/8MTZv3gwzMzPp/c2aNcOnn36KTz/9\nVO2rTFD18vPzMXv2bHz55Zfo0KFDjcc2adIEW7ZswbJly/Dw4UOFxnHp0iU4OTnBxsYGixcvVto8\njLi4uDoX4YMHD0Z4eLjK1+Tet28fxo8fX+OAgrm5OTp16oSIiAilxBAdHQ0PDw84Ojpi8eLF+O67\n72BoaKiUa5FisAgnIrVRVhGuiJHwpKQkLFy4EPfv38edO3cwdepUhIaGYteuXdi/fz/69u0r0wi+\ni4sLfvjhB3zwwQe4d+9evWKrD7FYjAULFmDy5Mno3r17pcfHjRsHoVCIffv2qSE6qoulS5eie/fu\nGD58eJ2Od3V1xdy5czFnzhyFbmc/b948ZGdnQyKRYP/+/QgKClLYucs9ffoUJSUldd4OXR3b2Gdn\nZ+PcuXMYPXp0rccqsyVl6dKl0g8fpaWluH79ulKuQ4qjq+4AtIGBgQGEQtk+rwgEAhQUFEBPTw+6\nupqRZqFQqBGfipmbmr0u+cnNzcWDBw/QvXt36Ovr1+k5dc2Ns7MzHj9+DIFAgEaNGskVX3p6eqVV\nTtasWYP27dtXuE+W3AwdOhTp6el47733EBQUhGbNmskVW3Xqkp9NmzahqKgIn376abXH/PDDDxg+\nfDiGDx+OFi1a1CsmTXlvNZT31YEDBxATE4MLFy7I9Nz58+fj+vXr+P7776vsEZc1P2KxGNnZ2RXu\ny8/PV+jvWigUIjExEa6urjAyMqrz8yZMmIDVq1fjww8/VEgcteVm7969GDhwYJ0+KPj6+mLZsmX1\nzlNVr538/PwKt4uKilTy3tPE95a2YLbqQJ7+LT09PZiZmSE/P1+hIw/1YWhoqBGz5Jmbmr0u+bl+\n/TqcnJxQVlZW53PLkhtra2vExMTU+WvsV9nb28Pc3BxZWVkAAHd3dzRt2rRSrLLmZuzYsbh37x7G\njRuHP/74Q6H/SNaWn9jYWGzcuBGnTp1CSUlJtTns0KEDxowZg6VLl2LTpk31iklT3lsN4X2VlJSE\n5cuX4+DBgxAKhTLndcOGDfD19UWPHj3g7e1d4TFZ8lNQUICvvvoKjRo1khZ+lpaW6N+/v0J/14aG\nhoiIiICTk5NM5+3evTtSUlIQGxuLTp061TuOmnIjkUiwZ88erFy5sk4xOjo6IikpCY8fP67XzpVV\nvXZmzZolXQu8adOmePfdd1Xy3lPHe8vc3Fwl11E2tqMQkVooqxWlXH37wgsLCyGRSDBlyhQsW7YM\nBw4cqLX3u678/f1hbW2NefPmVRptV5b8/HzMmTMHX375ZZ2WK1u4cCHCwsLUutU2/U9RURFmz56N\njz/+GA4ODnKdo2nTpti4cSMWLlyIZ8+eyXWOyMhI+Pr64vnz5wgPD8e+ffvg6emJmTNnKmU79Li4\nODg7O8v0HB0dHYwYMUIlEzSjoqJQWFiI3r171+l4PT099OzZE1evXlV4LD169ICpqSk2b96Mc+fO\ncblRLcAinIjUIiIiQqlFuKOjY71W+Vi5ciUmTZqEr7/+GnPmzIGxsbHCYhMIBNiwYQNycnKwatUq\nlWws9Nlnn6Fbt25455136nS8kZERvvrqKyxZsgRFRUVKjo5qs3r1arRv3x6TJ0+u13m8vLwwfvx4\nzJ8/X6bJtyKRCGvXrsX06dPx6aefYtOmTTA1NcUbb7whnS+hDLGxsTIX4cCLVVL++9//Kn2CcUBA\nAMaPHy9Ty6qy+sL379+PcePGYcSIEfVuIyPVYBFORCpVVlaGs2fPIjw8HG5ubkq7jr29vdyTMy9c\nuICYmBgsWLBAwVH9j76+Pnbs2IGrV69i+/btSrsOAJw4cQI3btzAl19+KdPzBg0aBDs7O2zZskVJ\nkckmPz8f27Ztw+bNm5GWlqbucFTmzJkzCA4Oxvr16xWynOfChQtRXFxc599rYmIi3n77bSQkJODs\n2bMYOnRohcffeOMNXLt2TeFL7z179gxFRUVy7SPg4OAAExMT3LhxQ6ExvSwvLw+BgYEYM2aMTM/r\n168fLl26pNAP3yKRCIcOHcKECRMUdk5SPhbhRKQyYrEY06dPx7Rp01BYWIgFCxYorR3DwcFBru3r\nCwoKsHTpUqxZs0bpk5pMTU2xd+9e/PLLL/jzzz+Vco3Hjx9j+fLl2LJli1yj+atWrcKuXbuQlJSk\nhOjqrqysDOPHj8eqVauwZs0aDBs2DM+fP1drTKqQnJyMTz75BD/99FOVOzHKQ1dXF5s3b8b27dtr\nXC6vrKwMW7duxahRozBlyhTs2bOnyhFWCwsLdOrUSeGj4TExMXBycpL7g8eoUaOUuo39sWPH0Lt3\nb5lHnTt27AihUKjQ91RwcDBsbGxga2ursHOS8rEIJyKVuXv3boXd7C5duqS0jWHk3b7++++/R9eu\nXStNXFMWKysr7NmzB0uXLkV4eLhCz11aWop58+Zh1qxZcHV1lTu++fPnY+nSpSppm6lOcnJyhTXW\nHz58iOjoaLXFowqlpaWYO3cuZs6cCU9PT4We28rKCuvWrcOHH36InJycSo8/evQIY8aMwZkzZ/Dn\nn39iwoQJNRbDAwYMwPnz5xUaY0xMjFytKOWGDx+OoKAgpU1ODAgIkGvkuXwL+0uXLqk9FlIvFuFE\npDLGxsaV/iFv3Lix0q4n6+TMhIQEHDx4UOXbfDs5OWHjxo2YMWMGIiIicOXKFZk/PFRl06ZN0NfX\nx6xZs+p1nvfffx9ZWVk4duxYvWOSR1RUFFauXFnhPqFQiFatWqklHmUr/8Cxbt06GBoaYvbs2Uq5\nzptvvomBAwdi8eLFSExMxIMHDyCRSHDgwAEMGTIEAwcOxOHDhysty1mVgQMH4sKFCwqNr75FuKWl\nJVxcXHD27FkFRvVCfHw80tPT5f6wrsgi/NGjR4iOjoafn59CzkeqwyKciFTGysoKy5Ytk9729/dH\nx44dlXY9WTbtKSsrg7+/P/z9/dG8eXOlxVSdN954A5MmTcKIESMwbtw49OvXr179rOHh4dizZw9+\n/PFHmfc5eJWuri6++eYbfPnll5XWhlYWsViMc+fOYdSoUZg1axa8vLywa9cu2NjYwNjYGF27dm2Q\nX72XtzgMGzYMv/zyC7744ot6//5qsnTpUly9ehV9+/aFtbU1+vTpg507d+KPP/7A7Nmz67wikKOj\nI/Ly8hRYBo74AAAgAElEQVS6EVV9lhgtp6xt7AMCAjBu3Di5V0zq06cPQkNDIRKJ6h3LgQMHMGLE\nCI1Yj59kwyKciFRq8ODBaNmyJe7evYv58+cr9VqyjITv3bsXenp6GD9+vFJjqsnDhw+lqzkUFBRg\nw4YNcp0nJycH8+bNw7p162BpaamQ2Dw8PPDmm2/im2++Ucj5qiMSiXDw4EEMHDgQ69atw6RJkxAS\nEoLp06fjzTffxJUrVxAWFoakpCQkJiYqNRZ1+Prrr1FaWgrgRTuKols8XhUZGYnc3Fzp7bt372L/\n/v2wt7eX6TxCoRD9+/fHxYsXFRJXZmYmcnNz6zQKX5PBgwcjIiJCoRN5CwsLcfz4cYwdO1buc1hY\nWKBjx44VWqzkUVpaigMHDrAVRUuxCCcilQoNDUWvXr1k2gFPXnUdCU9NTcWGDRuwdu1apY461sbA\nwKDC7fDwcHh5eeGjjz7CoUOH8Pjx41rPIZFI8PHHH8PHxwe+vr4Kje/TTz/F2bNn6104VOX58+fY\nunUrevXqhWPHjmHlypU4c+YMhg8fXmkXPlNTU8ydOxdr1qxReBzq9upEZUWshlKTqvr85d31UJF9\n4bGxsXBxcan3+7Fx48YYNGiQQrexP3nyJDw8POq0Q2ZNFLFU4cWLF9GqVSu5144n9WIRTqRBdu3a\nhb59++KNN97A3bt31R2OUty4cQM9evRQybVsbW3x6NGjWte5XrFiBSZPnozOnTurJK7q/N///Z90\n5K9ly5YIDAzEjh074OTkhHPnzsHPzw/du3fH/PnzsX//fty7d09aRJ0+fRojRoxA165dERsbi+XL\nlys8PlNTU3z22Wfw9/eXjtbKY/Xq1XB2dsbAgQNx+fJlrF69Gj179kRcXBz27NmD/fv3o1+/fjUW\noFOnTsXt27eVtj61OsTHx6OwsFBaBDs7Oyt9hNPLyws+Pj7S24sWLYKZmZlc5+rXrx/Cw8NRUFBQ\n77hiY2Plnkz8KkWvkhIQEICJEyfW+zyKKMIVFQupB7etJ9IQ165dw4oVKwC8WJd3ypQpCp09rylC\nQ0OVNtHsVfr6+ujQoQPu3LlTbW/p2bNncevWLWzcuFElMdWkTZs2uHjxIp48eYKWLVtKezzt7e0x\nbdo0SCQSJCUl4caNGwgJCcH69eulj1+6dEnaytKmTRul9Ye+8847OHDgAHbu3In//Oc/Mj//9OnT\n0vWpMzMzMXHiREybNg1BQUF12smznIGBAT755BN89dVXOHHihNJHjJUtLCwMM2bMwLp169CzZ0+k\np6fDxsYG+vr6Sr2ujo4Odu/ejb///htt2rRB06ZN5d563MTEBC4uLggJCcGgQYPqFVdcXBxGjBhR\nr3OU8/LywrNnz3D37t16b2N/584dPHz4sMIHF3l5enrin3/+QWZmJiwsLGR+fmpqKsLCwrB58+Z6\nx0LqwZFwIg3x77//Vrqt7N3eVC0lJQXPnz+v9z+EsqipLzw/Px/Lly/HN998ozGTmgwMDGBtbV1l\nPAKBALa2tpg0aRI2b96MiIgIHD16FK1bt67wWnn8+LFCRiOrIhAI8PXXX2PTpk1ITk6u03NEIhEi\nIiLw008/Yd26dRUe09fXxxdffCFTAV5uxIgRKC4uRmBgoMzP1STnz5/HBx98gE2bNmHo0KFo1qwZ\n7OzslF6AlxMKhXB1dYWdnV29z+Xj46OQVVLi4uLqPSmznI6ODoYPH66QCZoBAQEYPXo09PT06n0u\nAwMDdO/eHSEhIXI9/+DBg/Dz81PqClOkXCzCiTRE7969KxRevr6+au1PVobQ0FD06NFDpaOWNW1f\nv379evTs2RN9+/ZVWTyKJBAI0L59e8yZM6fCa8fd3V2pPfc2NjaYPn06Zs2ahfXr11f6Sj0/Px+X\nL1/Gt99+i2HDhsHR0RFLly5Famoq3nvvvQqbBr377rtyxyEUCrFs2TKsWbNG7tFbdTt27BgWLVqE\nX3/9Ff369VN3OPU2YMAAXLhwoV5rymdmZiIrK0uhKycNHDgQu3fvhr+/v9w76RYXF+PIkSMKnbzt\n7e0tV0uKWCzGgQMH2Iqi5diOQqQhbGxssGLFCnz++edo1aoVtm3bpu6QFE6V/eDl7O3tq5wsFh8f\nj6NHjyp8bWN1sLa2xsGDB7Fv3z5YWlrWe13wujAzM8PNmzdx8+ZNAMCcOXNQWlqKsLAwJCYmwsnJ\nCT169MC8efPg4uICExMT6XO9vb1x+vRptGjRol5FePm52rRpg4CAAEyZMqVe51K1nTt3YsOGDTh4\n8KBCRqE1QefOnSGRSHDnzh106dJFrnPEx8fDyclJYYMQEokEK1euRF5eHn7//XecPHkS58+fl3md\n+cDAQDg4OMDa2lohcQEvXr/bt2+HRCKRaXDi0qVLaNKkicK+LSD1YBFOpEFSUlLw/vvvY+/evVrf\n41qV0NBQTJo0SaXXLF8h5eV/5MrKyvDJJ59g6dKlaNq0qUrjURZPT0/07NkTzZs3R1pamtJHhk+f\nPl3h9qFDhzB16lSsWLECrq6u0pF5Q0PDSjsW2tjYYO7cuQqLZenSpZgyZQpGjRqlFV/NSyQS/Pjj\njzh06BCOHj1a72X4NIlAIJC2pMhbhMfFxcHJyUlhMWVmZiI+Pl56OycnBz///DNmzpyJNm3a1Pk8\nv//+u8JHnm1tbVFaWop///1XppH/3377rdZdTEnzNazvuom0XEhICN566y106tRJadu5q0t6ejqe\nPn2q8qW0mjdvDl1dXaSmpkrv2717N4yMjDBmzBiVxtKQvNrDPXbsWCxYsAA9e/ZUeX+9i4sLevfu\nrZRvj7KysnD+/HncuXNHIecTi8VYuXIl/vzzTwQGBjaoArxcfZcqLF+eUFHMzMwqrJcvFApx9+5d\n+Pn5oVu3bvjwww/x22+/ITExscp5OOnp6Th69Chu3bqFN998U2FxAS8+tHh7e+PKlSt1fk5GRgYu\nXryIkSNHKjQWUj0W4UQaIjc3F4mJidIRTWWsxaxOYWFh6Nq1q9w7zNXHy+uFJycn4/vvv8c333zD\nUaR6WL58OQYNGgRLS0sMHz4cCxYsUGs8/v7+2LlzJ9LT0xV2zuTkZAwcOBDvvfcefHx8cOjQoXqd\nr7S0FIsWLUJUVBQOHz6ssI2UNI2XlxdiY2MrbAIki/j4eIUW4To6Oti3bx+8vb3h6emJbdu2Yf/+\n/YiOjsbBgwfRp08fREVF4f3334ezszOmTp2Kn3/+GZGRkQgJCUH37t3x7rvvoqioqMKHeUWRdQv7\nQ4cOwc/Pr0KLF2knFuFEGiI0NBTu7u5o1KgRevTo0eCK8Bs3bqBnz55qufbLK6R89tlnmDZtWoPc\n8lyVzM3NsXv3bkRGRuKnn35S++oy7dq1w8iRI/H9998r7JwHDhyQFl1isRjr1q2rtJlOXRUVFWHm\nzJlIS0vDgQMH5F6LWxsYGRmhW7duck04zMnJQVpaGmxsbBQak52dHQICAnDixAkMHjwYwItRaBsb\nG4wbNw7ff/89QkJCcP78eYwcORIpKSlYsmQJxo0bh7y8PAAvJhz//PPPCo0LAPr27Yvr16/XqYVM\nIpEgICAAkydPVngcpHoswok0REhICLy8vACgQY6El6+Mog4ODg6IiYnB6dOncefOHYX2I5PmWLBg\nAY4fP15puU95vbpzZHp6OpycnDBp0iT8+OOPuHbtWqV+96o8f/4ckydPhoGBAX799VeV7BarbuWr\npMgqLi4ODg4OavnGDAAsLS0xbNgwfPnllzh79iy6detW4XFlrFjVtGlTWFtbIyoqqtZjQ0NDIRQK\n1fa3lBSLRTiRhni5CO/SpQuysrIU+tW6OuXm5uLevXtqmcn//PlzbN++HYGBgZg5cybmzJmDRo0a\nqTwOUj4LCwvMnDkTa9eurfe5nj9/jsuXL8Pc3BzAi91CDxw4gCtXrmDixInIycnB119/DWdnZwwd\nOhRffPEFTp8+LX3PHjx4EM7OznB2dsagQYNgY2ODzZs3q2ztb3UbMGAALl68KPNeB4pcH1wRlixZ\nIm37aNOmjdI2GqtrS8q+ffs4IbMB0ZrVUQoKCnDixAkkJSXByMgIPj4+Vb5RJRIJLly4gOjoaIhE\nIlhaWsLPzw8tWrQA8GK75JeVlpaiW7duGDJkiEp+DqKqZGZm4uHDh9JtmoVCITw8PHDz5k34+vqq\nObr6Cw8Ph5ubm1oKkG3btklXRhCLxTh8+LBC1/klzTJjxgxpj6+7u7tc50hPT8ekSZPg5uaGAwcO\nICMjA2ZmZtIPb4MHD5a2NBQWFiIqKgrh4eHYt2+fdNv3R48eSdfKzs7OxqJFi9Q2uqsOHTp0gLGx\nMRISEuDs7Fzn58XFxaF///5KjEw23bp1Q3h4OPLz82Fubq60v2H9+vXD2rVr8fHHH1d7TFZWFs6d\nO4dVq1YpJQZSPa0pwgMDA6Gjo4PFixcjNTUVAQEBsLS0lBbX5RISEqQTLMzMzHDhwgUcPXpUum7u\nsmXLpMcWFxdj/fr1Kl+tgehV169fR/fu3Svswubp6dlgivDQ0FC19YO/Ojns+fPnaomDVMPQ0BCL\nFi3C6tWrcejQIZlHDB8/fozx48dj2LBhWLx4MQQCQY0TKA0NDdG7d2/07t0bwIvlL48fP4558+ZJ\njxGLxcjMzETLli3l+6G0VPkqKbIU4bGxsZg/f74So5Kdubk5OnfurNSlP7t164bExERkZ2dXO1/g\n6NGj6N+/v1xb3JNm0op2FJFIhFu3bqF///4wMDBA+/bt0aVLF8TExFQ6Njs7G+3atYOFhQWEQiFc\nXFyQlpZW5Xlv376Nxo0bN8gloki7vNyKUq5r167SjVC0nTo26Sk3YcIEmJqaAnjxDcOMGTPUEgep\nztixY5GWliZzT/KdO3cwYsQITJkyBR9//LFcX/nr6OjAz8+vwjrXHh4er+VEYB8fH5mWKszNzcXT\np09fy1zVtoV9+YRM7pDZsGjFSHhGRgaEQiGaNWsmvc/S0hL379+vdKyTkxMSEhKQnp4Oc3NzREdH\nV/uGjo6Ohqura4U/tLm5udKZ0OVEIpHMG0CUT+h5dWKPOuno6FQYaVUX5qay69evY9KkSdDT05Pm\npVu3boiJiYFQKFT719j1yU9BQQFu375daaRfHvK8dpycnPDXX38hLCwMtra2Mo3K1YW6Xzsv43vr\nBT09PaxYsQJr1qzBoEGDoKOjU2tuIiMj8d5772HlypUYPXp0va9/4sQJHDp0CEKhEKNHj65yMmZD\nf+306dMHM2fORG5ubp02xUpMTISDg4N0pR1NyY+q3ldvvPEGrl69iuHDh1d6LCIiAsXFxfD29oZA\nINCY3ACa+XdHW2hFxkQiEQwMDCrcZ2BggOLi4krHGhsbo127dti8eTMEAgFMTU2r3Mo4OzsbDx48\nwDvvvFPh/sjIyEqTI7y9veXuUSuf1EOVMTcvPHnyBGlpaejfv3+FYrtjx45o1aqVdEUGbXX+/Hm4\nubkp9BsnWV87zZs3h5ubm8Kur+n43gImT56Mbdu2ISgoCFOnTpXeX1Vuzp49i8mTJ2P37t3w8/NT\nyPWbN2+OTz75RCHnUiVFv3YGDBiAyMjIOo3gJiUloUePHmjevLlCY1AUZb+vRo4ciWHDhlX58x8+\nfBj/+c9/KrXgahL+3ZGdVhTh+vr6lQruoqKiSoU5AFy6dAnJyclYuHAhjI2NERsbiz179mDOnDkV\nJlTExMSgXbt2lV40np6elbbaFYlE1ba0VEdXVxfm5ubIyspCaWmpTM9Vluo+uMijqKgIgYGB0NXV\nxeDBg2X6RN7QcyOr48ePo2fPnsjMzARQMT9ubm4IDg5Wey9pffJz+vRpdO3aVeb3UFX42qkZ81PR\nkiVL8J///AcDBgxAkyZNqszN8ePHsWTJEuzatQvdu3dXyOu0rl6H107fvn1x9OjROs1tCQkJgbe3\nt/R3oCn5UdX7qkWLFigoKEBYWBg6dOggvf/58+c4cuQIQkJCNC43gHr+7mjqBzVZaUUR3rRpU4jF\nYmRkZEi/0nr69GmVv4TU1FQ4OTlJe0Dd3d0RFBSEtLQ0WFlZSY+LiYlBnz59Kj3fxMSk0i5UKSkp\nck/GKC0tVdpEDlnp6uoqJBaRSITRo0cjIiICwIuv0Pbu3Svz+qkNMTfyuHLlCnr16lXp+qWlpXBz\nc0N4eDjGjh2rltjK1Sc/165dw5w5cxSaX752asb8vODu7g4XFxds27YN//d//wegYm727NmDjRs3\nYv/+/XBwcFB5nK/Da6dfv35YvXo1ioqKam2ri42NxezZs6XX17T8qOJ91bdvX5w/f77CN/iHDh2C\nl5cXzM3NNTY3gGb93dEWWjExU19fH/b29rh48SJEIhEePHiAxMRE6XJuL7OyskJCQgLy8vIgFosR\nExMDsVhcYTbxw4cP8fz5czg6Oqryx2gw4uLipAU4APz1118K2xzjdVTVpMxy5SukaKvi4mLExMSg\na9eu6g6FXlOffvopfv75Z2RlZUnvk0gk+OGHH/DLL7/gyJEjXCFLiaysrGBpaVnr37G8vDwkJyej\nU6dOKopMM3l7e1faaTQgIAATJkxQU0SkTFpRhAOAn58fSkpK8O233+LIkSPStb+zs7OxevVqZGdn\nAwC8vLxgaWmJrVu34ptvvsH169cxZsyYClsqx8TEwN7evsp2FqqdqalphcmsOjo6lb49oLp5/Pgx\n8vPzK7VAlbOzs8OjR48qLbOnSBkZGYiOjq40IVkRYmNj0bFjR74+SG1sbW0xZMgQrF+/Hvfv34dI\nJMLnn3+OP//8E//9739hbW2t7hAbPB8fn1pXqklISICdnZ3GTDZUl759++LatWvSto74+HhkZGSg\nX79+ao6MlEEr2lEAwMjIqMoNNszMzCqs/a2npwc/P78aJ9e8/fbbSonxdWFra4vly5fjq6++gkQi\nwddff63Rk0U0WUhICHr16lXtUmh6enpwdnZGdHS0Uv4IX79+HVOmTEF+fj5at26No0ePom3btgo7\n/40bN9S2PjhRuUGDBmHq1KnYvn07TE1N0bFjRxw5ckTatkjKNWDAAHz22Wfw9/ev9pi4uDiFr1yk\njZo3b442bdogKioK3bp1w759+zB+/Hi1r5BFyqE1I+GkWWbNmgVHR0fo6Oiw1aAeampFKefu7q60\nlpRvv/0W+fn5AF7Mffjll18Uen51btJDVG7jxo3S3StzcnLQo0cPFuAq5OnpicePHyM1NbXaY2Jj\nYzVqu3p18vb2xpUrV6Q7hat7ThApD4twktuzZ8/g7e1d7eYCVDOJRFKnItzDwwNRUVFKieHVEXhZ\nJ9fWpLS0FBEREejevbvCzkkkj1dXkSgrK1NTJK8nXV1d9O3bFxcvXqz2GI6E/0+/fv1w6dIlnDx5\nEp6enmjdurW6QyIlYRFOciktLUVmZibeeecdFuFyunfvHgBUWIqqKh4eHrh586Z0JE+R/P39pT2Y\nRkZG+M9//qOwc9+6dQutW7fmFsukdgsWLJC+zlu0aFFh3XBSjQEDBlTbF15QUICHDx+ic+fOKo5K\nM4nFYty8eRNLly7V6j0iqHYswkku5TuS9uvXD6GhoRxZkkP5KHhtW2O3atUKBgYGePDggcJj6Nq1\nK8zNzbFhwwbo6uqiUaNGCju3OreqJ3rZ4MGDcfXqVZw5cwaXL19W6MZRVDcDBgzA1atXIRKJKj2W\nkJCALl26VNjL43WVk5ODWbNmQSwWo6ioCFu3bkVKSoq6wyIlYRFOcnn27BlatGiBFi1aoHnz5khI\nSFB3SFqnLq0o5cpHwxUtJiYGpqamGDduHAYPHozff/9dYedmPzhpkg4dOsDX15ffzKhJs2bNYGNj\ng/Dw8EqPxcXFccT3/0tNTcXz58+lt4uLi5GcnKzGiEiZWISTXFJTU6W7OHp5ebElRUYSiQTXrl1T\nexEeFBSEN998EwAwffp0/Pbbb1WOVMlKLBYjNDSU/eBEJFVdSwonZf6PtbU1bG1tpbetrKxgZ2en\nxohImViEk1yePXvGIrweEhMTYWxsjDZt2tTpeGUV4WfPnpUW4Y6OjujQoQNOnTpV7/PevXsXpqam\naNWqVb3PRUQNw4ABA3D+/PlK98fHx7MI//8MDAxw5MgRfPjhh5g9ezb++9//okmTJuoOi5SERTjJ\npbwdBQB69uyJ8PBwblcrA1lGwQHA2dkZd+7cQWFhocJiSEpKQk5ODtzc3KT3zZgxAzt27Kj3JNDr\n16+zH5yIKnB1dUVmZiYePnwova+wsBD37t2rdsOy11GzZs2wZMkSLF++HFZWVuoOh5SIRTjJJTU1\nVVqEW1hYwNraGtHR0WqOSnuEhISgd+/edT7e0NAQnTp1Qnx8vMJiOHv2LHx9fSssSzhw4EBkZmYi\nMjKyXucODQ1lEU5EFQiFQvTv379CS8qtW7dga2vLHazptcQinOTy7NkzWFpaSm/37t2bLSl1VFZW\nhhs3bshUhAOKb0l5uR+8nI6ODt5//33s3LlT7vNKJBJOyiSiKr3aFx4XF8dWFHptsQgnubzcjgKw\nL1wWt27dQvPmzaU99XWlyCI8LS0Nd+7cqfKDwNixY3H58mW5l8W6f/8+BAIB2rVrV98wiaiB8fb2\nRmhoqLS1jpv00OuMRTjJ5enTpxWKyB49eiA6OhpFRUVqjEo7yNqKUk6RRXhwcDDeeOONKr8CNjEx\nwbvvvos9e/bIde7yUfDa1j8notePmZkZHBwccP36dQAvVkZhEU6vKxbhJLOysjJkZGSgefPm0vua\nNGkCOzu7evcSvw5kWR/8ZdbW1igsLERqamq9Y6iqFeVl06ZNQ0BAgFwTQW/cuMFWFCKqlo+PDy5c\nuICioiL8+++/sLe3V3dIRGrBIpxklpmZiSZNmlTa3Yx94bUrKSlBeHg4evXqJfNzBQIB3N3dERUV\nVa8Y8vPzERoaigEDBlR7TIcOHeDh4YEjR47IfH72gxNRTcr7wm/fvg0bGxuF7tRLpE1YhJPMXm1F\nKce+8NrFxMSgbdu2cu/ap4iWlL/++guenp4wMTGp8bgPPvgAO3fulGm5wuTkZOTl5VXYbIKI6GX2\n9vYoLi7GsWPH2IpCrzUW4SSz6orwbt264datW8jLy1P4NcViMdasWYOhQ4fik08+QX5+vsKvoQry\ntqKU8/T0rHcRXlsrSrk+ffpAKBTiypUrdT53WFgY+8GJqEYSiQStWrXCrl27kJWVBbFYrO6QiNSC\nRTjJ7NWVUcoZGhrC1dUVYWFhCr/m9u3bsXnzZkRFRWHfvn1YtWqVwq+hCrJu0vMqNzc3xMbGorS0\nVK7nl5SU4MKFC/D19a31WIFAgOnTp2PHjh11Pv+NGze4PjgR1Wjr1q2IioqCWCzG2bNnsXnzZnWH\nRKQWLMJJZi9v1PMqLy8vXLt2TeHXvH37do23tUFRURFu3rxZryLVxMQEbdq0wd9//y3X80NDQ2Ft\nbV3n7eRHjBiB6OhoJCUl1fn87AcnopqEh4fXeJvodcEinGT26kY9L1PW5Mx+/fpVuN21a1eFX6Mq\nZ8+excGDB5GZmVnvc928eRNdunSptRe7Nu7u7nKvQnPmzJk6taKUMzQ0xIQJE7Bt27Zaj01PT8fT\np0+50gER1cjNza3CbQ8PDzVFQqReuuoOgLTPs2fP0KdPnyofc3d3R1JSErKzs2FmZqawa44cORLr\n169H+/btIRaLkZSUBIlEotTe42XLlmH37t0AgDZt2uD06dNyT6gE6t+KUs7DwwPh4eGYMmWKTM+T\nSCQICgrC77//LtPzpkyZgoEDB2LhwoUwNTWt9riwsDB07doVOjo6Mp2fiF4vH374IcRiMSIiIuDh\n4YH58+erOyQitWARXgcGBgYQCmX70kAgEKCgoAB6enrQ1dWMNAuFQhgaGtb7PM+ePUPbtm2rPJeh\noSG6d++Omzdvws/Pr8rny5ObuLg4lJaW4siRIygrK8PgwYPx+++/Y+bMmfX6Wcq9mpuysjLs3btX\nevvx48e4fPkyxo8fL/c1rl+/jsWLF9f6O6gtP71798aOHTtk/l3GxsaiUaNGcHV1lenDi42NDQYO\nHIgjR45g7ty51R4XERGBvn37KuQ1Vp2G/L5SBOanesxNzVSdn6VLl9Z6jKbkh6+dmmlifrQFs1UH\nxcXFMj9HT08PZmZmyM/PR0lJiRKikp2hoaFcm6+8KjU1Febm5tWeq1evXvjrr7+qXYdantzs2rUL\n48aNg0gkAgBs2rQJw4YNg6enJxwcHOT7QV5SVW4aN26M3NzcCrflzV9BQQFiY2Ph6upa6zlqy0/7\n9u3x5MkTpKSkwNzcvM4xHD9+HL6+vnLtajpz5kxMnz4dU6ZMqXak++rVq/j6668V8hqrTkN+XykC\n81M95qZmzE/1mJuaqSM/svzbp8nYE04yEYvFSEtLq7Bb5qsU3RdeUFCA48ePY+zYsdL7OnTogM8+\n+wxz5sxRyh+itLQ0GBgYwNDQEEKhEEZGRvW6Tnh4OJydnWFkZFTv2HR0dODi4oLo6GiZnhcUFIS3\n3npLrmt27doVzZo1w9mzZ6t8PCcnB/fv34eLi4tc5yciInrdsAgnmWRlZaFx48Y17nDm7OyMlJQU\npKWlKeSaJ0+ehKenJ6ysrCrcP2rUKDg5OWHlypUKuU65wsJCTJs2DRMnTsQ///yDe/fu4dixY1i+\nfDlCQ0PlOmdISAh69+6tsBhl3bTn4cOHSEtLq9cEqBkzZmDnzp1VPhYeHg43N7dKu6gSERFR1ViE\nk0yq26jnZbq6uujRo4fClirct28fJk2aVOl+gUCANWvW4MqVKzh9+rRCrlVWVoa5c+fCxsYGixcv\nBvDi53F0dMSmTZswc+ZM/PPPPzKft76b9LxK1iL8zJkzGDRoUL0mTQ4ZMgT37t1DfHx8pce4NCER\nEZFsWIQ3AElJSfDx8YG1tTVmzJghVw97XT19+rTaNcJfpqgt7P/++28kJydX21/epEkTbNq0CZ9+\n+tw937YAACAASURBVClSUlLqdS2JRIKVK1ciLy8P69evrzR50dvbG0uXLsXkyZNlGuXPzc3F3bt3\nFboMl7u7u3Szi7qQdWnCqujp6WHq1KlVjobfuHGDRTgREZEMWIQ3AP7+/vj7779RUlKCwMBA7Nq1\nS2nXqm63zFcpatOegIAAjB07tsYZ156ennj//fcxf/58lJWVyX2t7du3IyQkBNu3b6+2rWLs2LEY\nPXo0pkyZgoKCgjqd98aNG3B3d4eBgYHcsb2qRYsWMDExwb///lvrsZmZmYiPj692WUlZTJw4EWfO\nnEF6err0voKCAvz9999wd3ev9/mJiIheFyzCG4CXCyIACuvFrsrTp0+r3ajnZXZ2dsjJyUFycrLc\n1yosLMTRo0frtCzghx9+CAByb3984sQJ/PLLL/jtt99qXAsbABYuXAg7OzvMnj27TtvHK7oVpVxd\nW1LOnTunsKUDLSwsMHTo0ArLN0ZGRsLR0VFjlssiIiLSBizCG4AJEyZI/79Ro0YYMWKE0q5V13YU\noVCIXr161Ws0PDAwEK6urmjbtm2tx+ro6GDjxo3YtWsXIiIiZLpOREQEPvroI+zevRtt2rSp9XiB\nQIC1a9dCJBJhxYoVkEgkNR6vqE16XlXXIlwRrSgve//99/Hbb79J255CQ0PRo0cPhZ2fiIjodcAi\nvAGYOXMmPvnkEwDAzp074ezsrLRr1bUdBah/X3hAQECFDxi1ad26NdauXYt58+ZVWN+7Jvfu3cOM\nGTOwZcsWmfKmp6eHbdu2ISIiAlu2bKn2uMzMTDx69EgpS/fVpQgvLCxESEgIfHx8FHZdOzs7dOnS\nBSdPngTAfnAiIiJ5sAhvINq1awcAMu/sKau6tqMA/+sLr22kuCr//PMPkpKS4OvrK9Pz3nrrLXh7\ne2PJkiW1XjczMxOTJ0/GokWLMGjQIJljbNKkCX777Tfs2bMHx44dq/KYa9euoVu3btDT05P5/LVx\ndHTEv//+W2Nv+pUrV+Ds7KzwjQ2mT5+OnTt3ori4GDExMejatatCz09ERNTQsQhvIHJycgC82F5d\nmerajgIAHTt2RFlZGR48eCDzdQICAjBmzBi5itfPP/8ct27dwqFDh6o9pnwtcD8/P0yePFnma5Rr\n1aoV9uzZg88++wzXr1+v9LiyWlEAwMDAAHZ2doiJian2mPps0FMTHx8fZGVlYdGiRWjVqhWaNGmi\n8GsQERE1ZCzCG4jc3Fzo6OgotQiXSCRIS0urdZ3wcgKBQK7dM4uLi3H48OE6TcisiqGhIbZs2YIv\nv/yyytVDxGIx5s+fDysrK/j7+8t1jZfZ29tjy5YtmDVrFu7cuVPhMWVNyizn4eGBqKioKh8rKytD\ncHCwQvvByz158gRZWVk4duwYkpKS8Msvvyj8GkRERA0Zi/AGIicnBx07dsSjR4+Udo3s7GzpVu51\nJU9feFBQEOzs7NChQwdZQ5Syt7fHokWLMHfuXIhEogqPffXVV8jIyMD333+vsPadPn36YMWKFZg8\neTKePn0KAEhNTUV6ejocHBwUco2qeHp6VtsXHhERgVatWtVpsqmsjh49iry8POntbdu2KfwaRERE\nDRmL8AYiNzcXjo6OSh0Jl6UVpZw8feH79u3DxIkTZQ2vkqlTp6Jly5ZYt26d9L5ff/0V586dw86d\nOxW6bjcAjBo1ChMmTMB7772HvLw8nDt3Dj179qzXLpW18fDwQGRkZJX5VVYrCoBK7SfGxsZKuQ4R\nEVFDVf0OKKRVcnJy4OHhgRs3bijtGs+ePatzK0q5tm3bwtDQEHfv3kXnzp1rPf7evXv4+++/FVI8\nCgQCfPfdd+jVqxd27NgBAwMD6Orq4vTp0wqfqFhu/vz5SExMhJubGwoLC9GsWTOkpKSgdevWSrle\nmzZtIJFIkJKSAisrK+n9EokEZ86cwfbt25Vy3QkTJuDChQs4f/48LCwssH79eqVch4iIqKHiSHgD\nkZubi86dOyMjI6NS+4WiPH36VOYiHIBMfeH79+/HqFGjFDZK/e+//yIvLw8lJSXIy8tDYWGhXD9D\nXQkEAggEAhQWFgJ4sZGSMgtUgUAgHQ1/WWJiIsRisdJaYfT19fHbb7/hzp07iImJQbdu3ZRyHSIi\nooaKRXgDkZubCwsLC7Rs2RIpKSlKuYY87ShA3fvCRSIR/vjjD5nWBq9NRkZGhdvFxcV13m5eXq+e\n//nz50q9nru7e6W+8KCgIPj6+kIgECj12o0bN1b6sphEREQNEf/1bCBycnJgYmKCNm3aKK0vXJ52\nFODFSPj169chFotrPC44OBgdO3aEra2tvCFWee2XJ3gOGTJEaa0o5aZPn45GjRoBeLGD6fvvv6/U\n61W1QsqZM2eU1g9ORERE9cee8AYiNzcXpqamSi3Cnz59Ck9PT5mfZ2lpiWbNmuHWrVtwcnKq9jhZ\nd8isiyZNmuDkyZMIDAyEsbExhg4dqtDzV6VPnz4IDg5GQkICnJyc6rXKS124uroiISEBIpEI+vr6\nSE5OxqNHj9C9e3elXpeIiIjkxyK8AZBIJMjNzUWTJk3Qtm1bpS1TKG87CvCiJeXq1avVFuGPHj1C\nTEwMduzYUZ8Qq2Rubq6Q1VZkYWNjAxsbG5Vcy9jYGNbW1rh16xbc3NwQHBwMHx8f6Ory7U1ERKSp\n2I7SABQUFEBPTw/6+vpo06aN0opwedtRgNonZ+7fvx8jR46UaQ1y+h8PDw9pX7gylyYkIiIixWAR\n3gDk5OTA1NQUwIslAZXRjiKRSOReHQV4UYSHh4ejpKSk0mOlpaU4ePCgwltRXiflm/bk5OQgKioK\n3t7e6g6JiIiIaqA131cXFBTgxIkTSEpKgpGREXx8fODi4lLpOIlEggsX/l979x4ddX3nf/w1M8kM\nk+tMLjAQEkDBiMYARqsVlKXIbpeA2rV1xa2H1l1bCtoup5y6p5zjerrn+Nsea627WNnt7lHcFq2u\n7IEK63pJml2r9gKES7CUy4Z7QhIyGXKdhJnfH2xGwjDfzCRz+w7Pxz9tJt/55pNXv9oXX97z+dap\nsbFRfr9fHo9HtbW1I8Yo9u3bp4aGBnV1dSkvL0/33Xefpk2blsxfJ66G58GlxJVwn88nm82m3Nzc\nMb2/qKhI5eXl2rt3r26//fYR36urq1NZWZmuv/76eCz1qjRv3jw9//zzqqur0x133KGcnJxULwkA\nABgwTQnfsWOHbDab1q1bp5aWFm3evFkejydsRrmpqUm7d+/WI488IpfLpbq6Om3ZskWrVq2SJB05\nckTvvfeevvjFL6qsrGzEo7fNyufzqaCgQNLFD0G2tbVpcHBQ2dnZcfsZ4xlFGTa8VeHlJfynP/1p\n0me2M83MmTPV3t6uF198UStXrkz1cgAAwChMUcL9fr8OHDig1atXy+FwaNq0aaqsrNSePXu0ZMmS\nEcd6vV5VVFSoqKhIklRdXa2PPvoo9P36+notXLhQ5eXlkhQqr8N8Pl9YMff7/THfAR7+UFwyPhzX\n09Mjl8ul7OxsZWdna+LEiWprawu7u2+z2cZczDs6OuTxeMZV7O+66y798z//s5544glJF7M5deqU\ndu7cqX/5l3+J6x8aYjWebOJtLNfOk08+qZ6eHjU1NenNN9/Ul7/85bhee+mSTzL/uYpWumQjkY8R\nsjFGPpGRjbF0zMcsTJFYR0eHrFarSkpKQq95PB41NzeHHVtVVaWmpia1t7fL7XarsbExtO90IBDQ\n6dOnVVlZqeeff15DQ0O6/vrr9cd//Mehi3nnzp1qaGgYcc6FCxdq0aJFY1p7ovekli7+XhMnTlRp\naamkiztznD9/PvR1PPT29qqiomJc51y+fLlWrVoV+vCl2+3WP/zDP+ihhx7S9OnT47TSzBHttXPu\n3Dlt3Lgx9PWvf/1r7d27V3/yJ3+SqKWlXDL+uTIz8omMbIyRT2RkY4x8YmeKEu73+8MeY+5wODQw\nMBB2bF5enioqKrRhwwZZLBYVFhaG/nq+u7tbgUBABw4c0COPPCKr1arXXntN//3f/63FixdLuvgB\nt8rKyrCf39bWFtOas7Ky5Ha71dnZqaGhoZjeG6uTJ0/K4XCE1ujxeLRv376w7QAjZRaNQ4cOyeVy\nxZzD5SorK/Xuu+9q+fLlam9v109+8hO98sor4z7veI0nm3iL9do5f/68rFbriIch9fX1xTXTdMkn\nmf9cRStdspHIxwjZGCOfyMjGWCryiedNxlQyRQm32+1hF1t/f39YMZekhoYGnTp1SmvXrlVeXp72\n7t2rTZs2afXq1aG73bfddpvy8/MlSZ/97GdHlPCCgoKwEZXTp09fcVePaAwNDY35vdE6d+6c8vPz\nQz+nrKxMzc3NYT83KytrzGs5c+aMPB7PuH+XO+64Qw0NDVq+fLneffddlZaW6vrrr094RqMZTzaJ\nEu21M2HCBD355JP63ve+p0AgoC9+8Yu69dZb4/r7pFs+yfjnKlrplo1EPkbIxhj5REY2xtIpH7Mw\nxRaFxcXFCgQC6ujoCL3W2tp6xT8JtbS0qKqqSoWFhbLZbJo3b17orqDT6Qwr2Jng0g9mSkrIUzPH\n86CeSw0/tEeS/u3f/o1tCePk0Ucf1e7du/Wb3/xGzz//vCwWS6qXBAAADJiihNvtds2ePVv19fXy\n+/06duyYDh48qDlz5oQdW1ZWpqamptDoyZ49exQIBEIf1Jw7d65+85vfqLu7W319ffr444913XXX\nJftXiqtLtyiUElPC47E7iiTdeuut2rdvnw4fPqwPP/xQ9957bxxWB0kqKSlRWVlZqpcBAACiYIpx\nFEmqra3V1q1b9cwzz8jpdIb2/vZ6vXrhhRe0Zs0auVwuzZ8/Xz09Pdq4caP8fr+Kior0wAMPhD4M\nuHDhQvX29uof//EflZWVpRtvvFF33nlnin+78bn8TngiHl0/ngf1XGpgYEB5eXm67bbbdNNNNykv\nLy8OqwMAADAX05TwnJwcrVixIux1l8ul9evXh77Ozs5WbW2tamtrr3gem82mZcuWadmyZQlba7J1\ndXWNKOGTJ0/W2bNnNTQ0FLctg+I1jvL444/r7NmzkqQPPvhAH3zwgRYsWDDu8wIAAJiJKcZRYOzS\nx9ZLF8d3SkpKdObMmbicv7u7W8FgMPRh1vHYtWvXiK9379497nMCAACYDSU8A1w+jiLFdy58eBQl\nHh/2q6mpCf13i8Uy4msAAICrhWnGURDZ5R/MlC6W8BMnTuizn/3suM8fr1EUSdqwYYOeffZZtbe3\na9myZbrjjjvicl4AAAAzoYSbXCAQ0Pnz58NGRcrLy+N2JzxeO6NIF/dhf/rpp1VaWqq2tjb2FAUA\nAFclxlFMrqenR06nM+wDmPHcISWed8IBAABACTe9K82DS5+Oo8RDvLYnBAAAwEWUcJPzer1h8+BS\n+o6jAAAAgBJuepHuhE+ZMkWtra0aGhoa989gHAUAACC+KOEmd6WdUaSLe4UXFRWptbV13D+DcRQA\nAID4ooSb3OVPy7xUvD6cyTgKAABAfFHCTS7SnXApPiW8t7dXg4ODEX8GAAAAYkcJN7lIM+FSfJ6a\n2draqtLS0rg8LRMAAAAXUcJNzuv1JrSEM4oCAAAQf5Rwk0v0OAo7owAAAMQfJdzkjEp4vMZRuBMO\nAAAQX5RwkzOaCS8rK9OZM2d04cKFMZ+fcRQAAID4o4SbnNEWhQ6HQ0VFRWppaRnz+RlHAQAAiD9K\nuMkZjaNIF++Gj2ckhXEUAACA+KOEm5zROIp08cOZ4ynhjKMAAADEHyXcxC5cuKDu7m7l5+dHPGbq\n1Knj2iGFO+EAAADxl5XqBZiBw+GQ1Rrbn1csFot6e3uVnZ2trKzExOz1epWXl6fc3NyIx1xzzTXa\ntWuXnE6nrFarnE5n1Ofv6+tTX1+fpkyZEteH9SQjm1jFmk0ikU9kZGOMfCIjG2PkExnZGEvHfMyC\ntKIwMDAQ83uys7PlcrnU09OjwcHBBKzq4l3qwsJC9fX1RTzG4/GoublZfX19cjqdhsde7vjx4yot\nLVV/f388lhuSjGxiFWs2iUQ+kZGNMfKJjGyMkU9kZGMsFfm43e6k/JxEYxzFxEabB5fG98FMdkYB\nAABIDEq4iRltTzhseK/wQCAQ8/lbW1vl8XjGujwAAABEMOYSXl9fr4aGhniuBTEabXtC6eJfWRUW\nFqq1tTXm8589e5Y74QAAAAkQdQlfuHChfvWrX0mSvv/97+vBBx/UQw89pKeffjphi4OxaO6ES2N/\nfD3jKAAAAIkRdQnfv3+/br/9dknST37yE9XX1+vjjz/Wxo0bE7Y4GIu2hJeXl49pm0LGUQAAABIj\n6t1RAoGALBaLjhw5omAwqBtuuEGS1NnZmbDFwZjP55PL5Rr1uLGWcMZRAAAAEiPqEr5gwQI99thj\nOnPmjL7whS9Iko4cOaKSkpKELQ7GfD6fpk+fPupxZWVl2rdvX8znZxwFAAAgMaIeR3n55ZflcrlU\nXV2tp556SpL0+9//Xt/61rcStTaMIpZxlLHOhDOOAgAAEH9R3wkvLi4O+xBmbW1t3BeE6EWzO4o0\ntnGUgYEBdXd3q6ioaKzLAwAAQARR3wkfGBjQ+vXrdc0114SK3zvvvKMNGzYkbHEwFs3DeqSL4yin\nT5+Oaa/wtrY2FRcXy2plK3kAAIB4i7phrV27Vvv379fPfvYzWSwWSdKNN96oF198MWGLg7Fox1Gc\nTqcKCgpi2iucURQAAIDEiXoc5T/+4z90+PBh5ebmhu6OlpWV6dSpUwlbHIx1dXVFNY4iXdwr/Pjx\n41HtpiKxMwoAAEAiRX0n3G63a2hoaMRrwyMLSI1oZ8Kl2B/Yw84oAAAAiRN1Cf/Sl76klStX6n//\n938lSWfOnNFjjz2mBx98MGGLQ2RDQ0Pq7+9Xbm5uVMeXl5fr+PHjUZ+fcRQAAIDEibqEP/3005ox\nY4Zuuukmeb1ezZo1S1OmTNHf/u3fJnJ9iMDn8yk/Pz80nz+a4XGUaDGOAgAAkDhRlfBAIKAPPvhA\nf//3f6/u7m61trbq/Pnzeu6552S32xO9RlxBLKMo0tjuhFPCAQAAEiOqEm61WnXvvffK4XBIkkpL\nS6O+A4vEiHZnlGFTp06Naa9wxlEAAAASJ+pxlLvuuksff/xxIteCGIylhJ88eTLqvcIZRwEAAEic\nqLconDZtmv70T/9U9957r8rLy0fcCf/e976XkMUhsljHUXJycpSXl6f29vZRy/Xg4KC8Xq9KSkrG\nu0wAAABcQdQlvK+vT/fdd58kjdjqjrGU1Ii1hEtSRUWFTpw4MWoJH9560mazjWeJAAAAiCDqEv7S\nSy8lch2j6u3t1bZt23TkyBHl5ORo8eLFqq6uDjsuGAyqrq5OjY2N8vv98ng8qq2tDRXPl156SSdP\nngw9cKigoECPP/54Un+XeIj2kfWXKi8v18mTJ1VTU2N4HKMoAAAAiRV1CZekQ4cO6dVXX9WpU6dU\nVlamFStWaNasWYla2wg7duyQzWbTunXr1NLSos2bN8vj8YSVxaamJu3evVuPPPKIXC6X6urqtGXL\nFq1atSp0zNKlS0ctouku1plw6dM74aNhZxQAAIDEivqDmb/4xS9UU1Oj3//+9yoqKtLBgwd1yy23\naNu2bYlcnyTJ7/frwIEDWrRokRwOh6ZNm6bKykrt2bMn7Fiv16uKigoVFRXJarWqurpabW1tCV9j\nssXyyPph5eXlUZdwdkYBAABInKjvhH/3u9/V1q1btWjRotBrv/zlL/XYY4/pnnvuScjihnV0dMhq\ntY74oKDH41Fzc3PYsVVVVWpqalJ7e7vcbrcaGxs1c+bMEce8//77eu+991RSUqLPfe5zmjFjRuh7\nPp9P3d3dI473+/1RP5lyWFZW1oj/jLfz58/L7XYrOzs76vdMnz5db7/99qjvaW9vl8fjiencsUh0\nNmNhs9kS9vvGinwiIxtj5BMZ2Rgjn8jIxlg65mMWUSd28uRJ3XnnnSNeW7BgwYgPaSaK3+8P7VE+\nzOFwaGBgIOzYvLw8VVRUaMOGDbJYLCosLNTKlStD31+yZIlKS0tls9m0f/9+vfrqq1q1apWKiook\nSTt37lRDQ8OIcy5cuHDEHz5i4Xa7x/S+0fT392vatGkqLS2N+j033HCDTp8+Pep7urq6VFNTE9O5\nxyJR2WQK8omMbIyRT2RkY4x8IiMbY+QTu6hL+Ny5c/Xss8/qiSeeCL32wx/+UHPnzk3Iwi5lt9vD\nCnd/f39YMZekhoYGnTp1SmvXrlVeXp727t2rTZs2afXq1bLb7Zo6dWro2Llz52rfvn06dOiQbrvt\nNklSTU2NKisrR5zT7/fHPNKSlZUlt9utzs5ODQ0NxfTeaLS1tSkYDMa0LrfbrWPHjuns2bOGu9oc\nO3ZMd911V8LGeBKdzVhE+kNdKpBPZGRjjHwiIxtj5BMZ2RhLRT6JvkmYLFGX8BdffFHLly/X888/\nH5otzsnJ0S9+8YtErk+SVFxcrEAgoI6ODhUXF0u6OLd8pf8RWlpaVFVVFZqXnjdvnt5++221tbWp\nrKws7HiLxaJgMBj6uqCgIOwDj6dPn9bg4OCY1j40NDTm9xrp6upSbm5uTOd2Op1yOp06c+aM4QXc\n0tKi4uLihKz7UonKZiyysrLSZi3DyCcysjFGPpGRjTHyiYxsjKVTPmYR9Qczr7/+en3yySd6/fXX\n9e1vf1uvv/66PvnkE82ePTuR65N08U747NmzVV9fL7/fr2PHjungwYOaM2dO2LFlZWVqampSd3e3\nAoGA9uzZo0AgoKKiIvX19enw4cMaHBzUhQsXtHfvXh07dixsZtwMxrJFoRTd4+vZHQUAACCxor4T\n3tjYqOLiYi1YsCD02okTJ3Tu3LkrluF4q62t1datW/XMM8/I6XSG9v72er164YUXtGbNGrlcLs2f\nP189PT3auHGj/H6/ioqK9MADD8jpdKqnp0d1dXVqb2+XxWJRSUmJHnzwQVM+GXIsu6NInz6+/uab\nb77i9y9cuKBz585lzF/1AAAApKOoS/iXv/zlsO0I/X6/Hn74Ye3duzfuC7tcTk6OVqxYEfa6y+XS\n+vXrQ19nZ2ertrZWtbW1Ycfm5ubqa1/7WkLXmQx+v1+Dg4NyOp0xv3f4gT2RtLe3y+Vypc2nrgEA\nADJR1OMox48f1zXXXDPitWuvvfaK2wQisYZHUYw+XBnJaHuFM4oCAACQeFGX8KlTp2rXrl0jXtu1\na5emTJkS90XB2FhHUaRPx1Ei4UE9AAAAiRf1OMratWt177336jvf+Y6uvfZaHT58WM8+++yIURAk\nh8/nG1cJN7oTfvbsWe6EAwAAJFjUJfzRRx+Vy+XSv/7rv+rkyZMqLy/XD3/4Q91///2JXB+uYKw7\no0if3gkPBoNXHGdhHAUAACDxRh1H2blzp/bv3y9J+tKXvqRXXnlF1dXVOnXqlN55552wR7wj8bq6\nusZcwvPz8+VwOHTu3Lkrfr+1tVWTJk0az/IAAAAwilFL+F//9V+rpaUl9PXXvvY1HT58WF//+tfV\n1NSk73znOwldIMKNZyZcMv5w5tmzZynhAAAACTZqCf/kk0905513SpK8Xq+2b9+un/70p1qzZo1e\nffXVpDwxEyONZxxFMi7h3AkHAABIvFFL+NDQkOx2uyTp448/1uTJk3XddddJuljmvF5vYleIMOP5\nYKZ08amikXZIoYQDAAAk3qgl/MYbb9Qbb7whSXrttdd09913h7536tSpcZVBjM14ZsKlyHfCA4GA\nOjo6eFomAABAgo26O8r3v/99LV++XKtWrZLNZtMHH3wQ+t7Pf/5zzZ8/P6ELRLjx3gkvLy9XQ0ND\n2OsdHR3Kz88P/c0HAAAAEmPUEr5gwQIdP35cf/jDH3TdddcpPz8/9L3a2lo9+OCDCV0gwo13JjzS\nA3sYRQEAAEiOqPYJz8/PV01NTdjrlZWVcV8QRjfecZThB/Zcvlc4D+oBAABIjqgfW4/0Md4tCgsK\nCmS329XZ2Tnide6EAwAAJAcl3ITGOxMuXdwh5fIPZ/K0TAAAgOSghJvQeGfCpSvvkMKDegAAAJKD\nEm4y/f39kqQJEyaM6zxX+nAm4ygAAADJQQk3mXjcBZcu3gm/UglnHAUAACDxKOEmE88SfqWZcO6E\nAwAAJB4l3GS8Xm9cSvjlj64PBoNqb2/nTjgAAEASUMJNJh47o0if3gkPBoOSpM7OTuXk5Ix71hwA\nAACji+phPVc7h8MhqzW2P69YLBb19vYqOztbWVnxi7m/v19FRUVyOp0xv9dqtYbe53Q6ZbPZNDAw\nILfbLa/Xq0mTJo3pvLFKVDbjcWk2qUY+kZGNMfKJjGyMkU9kZGMsHfMxC9KKwsDAQMzvyc7Olsvl\nUk9PjwYHB+O2lra2NuXm5qqvry/m9zqdzhHvmzp1qg4dOqSbbrpJx48f18SJE8d03lglKpvxuDyb\nVCKfyMjGGPlERjbGyCcysjGWinzcbndSfk6iMY5iMvEaR5FGfjiTnVEAAACShxJuMvHaHUW6eCf8\n0hLOzigAAADJQQk3ma6urriW8OEdUnhaJgAAQPJQwk0mniWccRQAAIDUoISbjM/nk8vlisu5Ln1q\nJuMoAAAAyUMJN5l4z4SfPHlSwWCQcRQAAIAkooSbTDzHUQoLCxUMBuX1einhAAAASUQJN5l4blFo\nsVhUXl6u/fv3y263p83G/wAAAJmOEm4iwWBQPp9P+fn5cTvn1KlTtWvXLu6CAwAAJBEl3ET6+vpk\ns9nkcDjids7y8nLt3LmTnVEAAACSiBJuIl1dXXEbRRk2depU7dy5kzvhAAAASUQJN5F4zoMPKy8v\nl9frpYQDAAAkESXcROK5PeGwqVOnShLjKAAAAElECTeReG5PKEmBQECvvvqqJOmNN97QmTNn4nZu\nAAAAREYJN5F4j6Ns3rxZr7zyiiTpk08+0bp16+J2bgAAAERGCTeReN8JP3bs2Iivm5ub43Zuzud4\nfwAAG29JREFUAAAAREYJN5F4l/AlS5YoKysr9PXnP//5uJ0bAAAAkWWNfgjShc/ni+sHKD/zmc/o\n3//93/Xuu+9qxowZevDBB+N2bgAAAERGCTcRn8+nmTNnxvWct956q2699da4nhMAAADGGEcxkXiP\nowAAACA1KOEmkoh9wgEAAJB8phlH6e3t1bZt23TkyBHl5ORo8eLFqq6uDjsuGAyqrq5OjY2N8vv9\n8ng8qq2tDZul7ujo0I9//GPdcMMNuv/++5P1a4xLIp6YCQAAgOQzzZ3wHTt2yGazad26dfqzP/sz\nbd++XWfPng07rqmpSbt379ZXv/pVPfHEEyovL9eWLVvCjtu+fbvKysqSsfS4YRwFAAAgM5jiTrjf\n79eBAwe0evVqORwOTZs2TZWVldqzZ4+WLFky4liv16uKigoVFRVJkqqrq/XRRx+NOGbfvn2aMGGC\nSktLde7cuRHf8/l86u7uDvv5ubm5Ma15eOu/S7cAHC+fz6eSkhJlZ2eP6f02m23M742nRGQzXumS\njUQ+RsjGGPlERjbGyCcysjGWjvmYhSkS6+jokNVqVUlJSeg1j8dzxYfLVFVVqampSe3t7XK73Wps\nbByxo0h/f7/q6+u1cuVK7dq1K+z9O3fuVENDw4jXFi5cqEWLFo1p7W63e0zvu1wwGJTP59O1116b\nMRd6vLLJVOQTGdkYI5/IyMYY+URGNsbIJ3amaHN+v18Oh2PEaw6HQwMDA2HH5uXlqaKiQhs2bJDF\nYlFhYaFWrlwZ+n59fb1uvvnmiLPVNTU1qqysDPv5bW1tMa05KytLbrdbnZ2dGhoaium9V9Ld3a0J\nEyaos7NzzOeIlFmyxTubeEiXbCTyMUI2xsgnMrIxRj6RkY2xVORTWlqalJ+TaKYo4Xa7Pexi6+/v\nDyvmktTQ0KBTp05p7dq1ysvL0969e7Vp0yatXr1aHR0dOnr0qL7+9a9H/FkFBQVhc9enT5/W4ODg\nmNY+NDQ05vdeqr29XQUFBeM6V1ZWVlzWEi/xyiYe0i0biXyMkI0x8omMbIyRT2RkYyyd8jELU5Tw\n4uJiBQIBdXR0qLi4WJLU2tp6xT8JtbS0qKqqKnSne968eXr77bfV1tam48ePy+v16rnnnpN08Q53\nMBjUxo0btWrVquT9QmPAzigAAACZwxQl3G63a/bs2aqvr9c999yjM2fO6ODBg/rLv/zLsGPLysrU\n1NSkqqoq5eTkaN++fQoEAioqKlJpaamqqqpCx3744Yfyer1atmxZMn+dMWFnFAAAgMxhihIuSbW1\ntdq6daueeeYZOZ3O0N7fXq9XL7zwgtasWSOXy6X58+erp6dHGzdulN/vV1FRkR544AE5nU5JFwv9\nMLvdrqysrJh3PkkFHtQDAACQOUxTwnNycrRixYqw110ul9avXx/6Ojs7W7W1taqtrR31nGPd8SQV\nurq6GEcBAADIEKZ5WM/VjplwAACAzEEJNwnGUQAAADIHJdwk+GAmAABA5qCEmwQz4QAAAJmDEm4S\njKMAAABkDkq4SXAnHAAAIHNQwk2C3VEAAAAyByXcJBhHAQAAyByUcJOghAMAAGQOSrgJBAIBdXd3\nKz8/P9VLAQAAQBxQwk3g/PnzysnJkc1mS/VSAAAAEAeUcBNgFAUAACCzUMJNgO0JAQAAMgsl3ATY\nnhAAACCzUMJNgHEUAACAzEIJN4Guri5KOAAAQAahhJsAJRwAACCzUMJNgJlwAACAzEIJNwFKOAAA\nQGbJSvUCzMDhcMhqje3PKxaLRb29vcrOzlZW1vhi7unpUUlJiZxO57jOY7Vax32OeIhnNvGSLtlI\n5GOEbIyRT2RkY4x8IiMbY+mYj1mQVhQGBgZifk92drZcLpd6eno0ODg4rp9/7tw5OZ1O9fX1jes8\n8ThHPMQzm3hJl2wk8jFCNsbIJzKyMUY+kZGNsVTk43a7k/JzEo1xFBNgi0IAAIDMQgk3AXZHAQAA\nyCyUcBPgsfUAAACZhRJuAuyOAgAAkFko4WluaGhIfX19ys3NTfVSAAAAECeU8DR3/vx55efnx7xF\nIgAAANIXzS7NsTMKAABA5qGEpzlKOAAAQOahhKc5r9dLCQcAAMgwlPA0x84oAAAAmYcSnuYo4QAA\nAJmHEp7meFomAABA5qGEpzk+mAkAAJB5KOFpjnEUAACAzEMJT3OMowAAAGQeSniao4QDAABkHkp4\nmvP5fHK5XKleBgAAAOKIEp7m+GAmAABA5qGEpznGUQAAADIPJTzNsTsKAABA5qGEp7HBwUH5/X7l\n5OSkeikAAACIo6xULyBavb292rZtm44cOaKcnBwtXrxY1dXVYccFg0HV1dWpsbFRfr9fHo9HtbW1\nmjhxoiTpzTff1NGjRzU4OKi8vDzNnz9fNTU1yf51ouLz+ZSfny+LxZLqpQAAACCOTFPCd+zYIZvN\npnXr1qmlpUWbN2+Wx+MJlethTU1N2r17tx555BG5XC7V1dVpy5YtWrVqlSRpwYIFWr58uex2u9ra\n2vTyyy9r8uTJmjJlSip+LUNdXV2MogAAAGQgU4yj+P1+HThwQIsWLZLD4dC0adNUWVmpPXv2hB3r\n9XpVUVGhoqIiWa1WVVdXq62tLfT9SZMmyW63S5IsFossFovOnTuXtN8lFsyDAwAAZCZT3Anv6OiQ\n1WpVSUlJ6DWPx6Pm5uawY6uqqtTU1KT29na53W41NjZq5syZI45566231NjYqKGhIXk8Hs2aNSv0\nPZ/Pp+7u7hHH+/1+5ebmxrTmrKysEf85Fj09PSosLFR2dvaYz3Epm80Wt3ONRzyyibd0yUYiHyNk\nY4x8IiMbY+QTGdkYS8d8zMIUifn9fjkcjhGvORwODQwMhB2bl5eniooKbdiwQRaLRYWFhVq5cuWI\nY5YtW6alS5fqxIkTam5uHnHh7Ny5Uw0NDSOOX7hwoRYtWjSmtbvd7jG9b9jEiRNVWlo6rnOkq/Fm\nk+nIJzKyMUY+kZGNMfKJjGyMkU/sTFHC7XZ7WOHu7+8PK+aS1NDQoFOnTmnt2rXKy8vT3r17tWnT\nJq1evTo0hiJJVqtV06ZN0969e/Xb3/5Wt99+uySppqZGlZWVI87p9/tHjLREIysrS263W52dnRoa\nGorpvcNOnDihCRMmxPyzI4n0B5dki0c28ZYu2UjkY4RsjJFPZGRjjHwiIxtjqcgnU25OmqKEFxcX\nKxAIqKOjQ8XFxZKk1tbWK/6P0NLSoqqqqtAs9bx58/T222+rra1NZWVlYccHAgF1dnaGvi4oKAh7\nOM7p06c1ODg4prUPDQ2N+b3nzp1TXl7emN9/uaysrLidKx7Gk028pVs2EvkYIRtj5BMZ2Rgjn8jI\nxlg65WMWpvhgpt1u1+zZs1VfXy+/369jx47p4MGDmjNnTtixZWVlampqUnd3twKBgPbs2aNAIKCi\noiJ1d3dr3759GhgYUCAQ0OHDh7V//37NmDEjBb/V6HhaJgAAQGYyxZ1wSaqtrdXWrVv1zDPPyOl0\nhvb+9nq9euGFF7RmzRq5XC7Nnz9fPT092rhxo/x+v4qKivTAAw/I6XSqp6dHv/vd7/TWW28pGAzK\n5XLp85//vK6//vpU/3pX5PP5NHny5FQvAwAAAHFmmhKek5OjFStWhL3ucrm0fv360NfZ2dmqra1V\nbW1t2LG5ubn66le/mtB1xhNbFAIAAGQmU4yjXK18Ph/jKAAAABmIEp7GmAkHAADITJTwNMY4CgAA\nQGaihKcx7oQDAABkJkp4GuNOOAAAQGaihKep/v5+BQIBTZgwIdVLAQAAQJxRwtPU+fPnVVBQIIvF\nkuqlAAAAIM4o4WmKeXAAAIDMRQlPU8yDAwAAZC5KeJriTjgAAEDmooSnKZ6WCQAAkLko4Wmqq6uL\ncRQAAIAMRQlPU8yEAwAAZC5KeJpiHAUAACBzUcLTFB/MBAAAyFyU8DTFTDgAAEDmooSnKcZRAAAA\nMhclPE3xwUwAAIDMRQlPU8yEAwAAZC5KeJriTjgAAEDmsgSDwWCqF5HuOjo6ZLXG9ucVi8Uiu90u\nv9+vWCMOBoOaPHmympubNWHChJjea8RqtSoQCMTtfGM1nmwSJV2ykcjHCNkYI5/IyMYY+URGNsZS\nkY/b7U7Kz0m0rFQvwAwGBgZifk92drZcLpd6eno0ODgY03v7+vpktVoVDAbV19cX88+OxOl0xvV8\nYzWebBIlXbKRyMcI2Rgjn8jIxhj5REY2xlKRT6aUcMZR0hDz4AAAAJmNEp6GmAcHAADIbJTwNMSd\ncAAAgMxGCU9D3AkHAADIbJTwNMTTMgEAADIbJTwNMY4CAACQ2SjhaYgSDgAAkNko4WmImXAAAIDM\nRglPQ5RwAACAzEYJT0OMowAAAGQ2Snga4k44AABAZqOEpyG2KAQAAMhslPA0xDgKAABAZqOEp6Gu\nri7GUQAAADIYJTzNBINBnT9/njvhAAAAGYwSnmZ6e3tlt9uVnZ2d6qUAAAAgQSjhaYZ5cAAAgMxH\nCU8zbE8IAACQ+SjhaYbtCQEAADIfJTyNBINB/epXv5Lf79fg4GCqlwMAAIAEoYSnkTVr1ugHP/iB\n9uzZo4ceekhDQ0OpXhIAAAASICvVC4hWb2+vtm3bpiNHjignJ0eLFy9WdXV12HHBYFB1dXVqbGyU\n3++Xx+NRbW2tJk6cqKGhIW3fvl1Hjx5VX1+f3G637r77bs2aNSsFv9FIx48f19atW0Nff/jhh9q1\na5c+85nPpHBVAAAASATTlPAdO3bIZrNp3bp1amlp0ebNm+XxeDRx4sQRxzU1NWn37t165JFH5HK5\nVFdXpy1btmjVqlUKBAIqKCjQV77yFRUWFurQoUN644039I1vfENutztFv9lFTqdTVqtVgUAg9Fpu\nbm4KVwQAAIBEMUUJ9/v9OnDggFavXi2Hw6Fp06apsrJSe/bs0ZIlS0Yc6/V6VVFRoaKiIklSdXW1\nPvroI0mS3W7XokWLQsdWVlbK5XLpzJkzoRLu8/nU3d0d9vNjLcRZWVkj/nM0U6ZM0d/93d/pySef\n1IULF/TNb35Tc+fOjelnjsZms6XF/uOxZpMM6ZKNRD5GyMYY+URGNsbIJzKyMZaO+ZiFKRLr6OiQ\n1WpVSUlJ6DWPx6Pm5uawY6uqqtTU1KT29na53W41NjZq5syZVzxvd3e3Ojo6VFpaGnpt586damho\nGHHcwoULR5T3WMRyh/273/2uvvWtb+nChQtXxQ4pqf7bh3RHPpGRjTHyiYxsjJFPZGRjjHxiZ4oS\n7vf75XA4RrzmcDg0MDAQdmxeXp4qKiq0YcMGWSwWFRYWauXKlWHHXbhwQW+++abmzp07ooTX1NSo\nsrIy7Oe3tbXFtOasrCy53W51dnaO6QOWsf68aETKLNnGm00ipEs2EvkYIRtj5BMZ2Rgjn8jIxlgq\n8rm0t5mZKUq43W4Pu9j6+/vDirkkNTQ06NSpU1q7dq3y8vK0d+9ebdq0SatXr5bdbpckBQIBbdmy\nRTabTUuXLh3x/oKCgrC70KdPnx7zloFDQ0Nps91gVlZW2qxFIpvRkE9kZGOMfCIjG2PkExnZGEun\nfMzCFFsUFhcXKxAIqKOjI/Raa2vrFf8k1NLSoqqqKhUWFspms2nevHnq6+sL3VkOBoPatm2benp6\n9Od//uey2WxJ+z0AAAAAySQl3G63a/bs2aqvr5ff79exY8d08OBBzZkzJ+zYsrIyNTU1qbu7W4FA\nQHv27FEgEAh9UPOtt95SW1ubVqxYkTYfagAAAMDVxRTjKJJUW1urrVu36plnnpHT6Qzt/e31evXC\nCy9ozZo1crlcmj9/vnp6erRx40b5/X4VFRXpgQcekNPplNfr1c6dO2Wz2fSDH/wgdO7ly5dfcc9x\nAAAAIBFMU8JzcnK0YsWKsNddLpfWr18f+jo7O1u1tbWqra294rFPPfVUIpcJAAAAjMoU4ygAAABA\nJqGEAwAAAElGCQcAAACSjBIOAAAAJBklHAAAAEgySjgAAACQZJRwAAAAIMko4QAAAECSUcIBAACA\nJKOEAwAAAElGCQcAAACSjBIOAAAAJBklHAAAAEgySjgAAACQZJRwAAAAIMko4QAAAECSUcIBAACA\nJKOEAwAAAElGCQcAAACSzBIMBoOpXkS66+jokNUa259XLBaL7Ha7/H6/0iViq9WqQCCQ6mWQzSjI\nJzKyMUY+kZGNMfKJjGyMpSIft9udlJ+TaFmpXoAZDAwMxPye7OxsuVwu9fT0aHBwMAGrip3T6VRf\nX1+ql0E2oyCfyMjGGPlERjbGyCcysjGWinwypYQzjgIAAAAkGSUcAAAASDJKOAAAAJBklHAAAAAg\nySjhAAAAQJJRwgEAAIAko4QDAAAASUYJBwAAAJKMEg4AAAAkGSUcAAAASDJKOAAAAJBklHAAAAAg\nySjhAAAAQJJRwgEAAIAko4QDAAAASUYJBwAAAJKMEg4AAAAkmSUYDAZTvYhM5PP5tHPnTtXU1Kig\noCDVy0krZGOMfCIjG2PkExnZGCOfyMjGGPmMHXfCE6S7u1sNDQ3q7u5O9VLSDtkYI5/IyMYY+URG\nNsbIJzKyMUY+Y0cJBwAAAJKMEg4AAAAkGSUcAAAASDLbU0899VSqF5GJgsGg7Ha7pk+fLofDkerl\npBWyMUY+kZGNMfKJjGyMkU9kZGOMfMaO3VEAAACAJMtK9QIyUW9vr7Zt26YjR44oJydHixcvVnV1\ndaqXlRZeeuklnTx5UlbrxUmogoICPf744yleVer8+te/VmNjo86ePauqqip94QtfCH3v6NGj2r59\nu7q6ujR16lTdd999crlcKVxtckXKprOzU88//7yys7NDxy5YsEALFy5M1VKTbmhoSNu3b9fRo0fV\n19cnt9utu+++W7NmzZLEtWOUD9eP9Oabb+ro0aMaHBxUXl6e5s+fr5qaGklcO1LkfLh2PtXR0aEf\n//jHuuGGG3T//fdL4toZC0p4AuzYsUM2m03r1q1TS0uLNm/eLI/Ho4kTJ6Z6aWlh6dKloX/hX+3y\n8/N111136ciRIxocHAy93tPTo5///Oe65557dN1116m+vl5vvPGGHn300RSuNrkiZTPsb/7mb2Sz\n2VKwstQLBAIqKCjQV77yFRUWFurQoUN644039I1vfEN2u/2qv3aM8hl2NV8/CxYs0PLly2W329XW\n1qaXX35ZkydPVmFh4VV/7UiR83E6nZKu7mtn2Pbt21VWVhb6mv/PGhs+mBlnfr9fBw4c0KJFi+Rw\nODRt2jRVVlZqz549qV4a0tANN9yg2bNnh/7lPuyTTz5RaWmpbrzxRmVnZ+uP/uiP1Nraqra2thSt\nNPkiZQPJbrdr0aJFcrvdslqtqqyslMvl0pkzZ7h2ZJwPpEmTJslut0uSLBaLLBaLzp07x7XzfyLl\ng4v27dunCRMmaMaMGaHXuHbGhjvhcdbR0SGr1aqSkpLQax6PR83NzalbVJp5//339d5776mkpESf\n+9znRvyDjIva2trk8XhCX9vtdhUVFamtrU2lpaUpXFn6+NGPfiRJuvbaa7VkyRLl5uameEWp093d\nrY6ODpWWlup3v/sd185lLs1n2NV+/bz11ltqbGzU0NCQPB6PZs2apbq6Oq6d/3OlfHp7eyVd3ddO\nf3+/6uvrtXLlSu3atSv0Ov+fNTaU8Djz+/1hnw52OBwaGBhI0YrSy5IlS1RaWiqbzab9+/fr1Vdf\n1apVq1RUVJTqpaUVv9+vnJycEa9xHV2Uk5OjRx99VB6PR319fdq+fbu2bNmihx9+ONVLS4kLFy7o\nzTff1Ny5c1VaWsq1c5nL8xkYGOD6kbRs2TItXbpUJ06cUHNzs7Kysrh2LnGlfPh3j1RfX6+bb75Z\nhYWFI17n2hkbxlHizG63h110/f39bNvzf6ZOnSqHw6GsrCzNnTtX5eXlOnToUKqXlXa4jiJzOBwq\nKyuTzWZTXl6eli5dqiNHjlyV/7IPBALasmWLbDabli5dKolr51JXyofr51NWq1XTpk2Tz+fTb3/7\nW66dy1yez9V+7Zw5c0ZHjx7V7bffHvY9rp2x4U54nBUXFysQCKijo0PFxcWSpNbWVv46JgKLxSJ2\nyQxXWlo64nMEfr9fnZ2dXEdXYLFYJOmqu46CwaC2bdumnp4e/cVf/EXog2JcOxdFyudyV+v1c6lA\nIBC6Rrh2wg3nc7mr7dppbm6W1+vVc889J+ni9REMBrVx40bdcsstXDtjwJ3wOLPb7Zo9e7bq6+vl\n9/t17NgxHTx4UHPmzEn10lKur69Phw8f1uDgoC5cuKC9e/fq2LFjmjlzZqqXljIXLlzQ4OCggsGg\ngsFgKJvZs2fr7NmzOnDggAYHB/XLX/5SkyZNuqr+hRYpm5MnT6q9vV2BQEC9vb36z//8T02fPl0T\nJkxI9ZKT6q233lJbW5tWrFgxYss0rp2LIuVztV8/3d3d2rdvnwYGBhQIBHT48GHt379fM2bM4NqR\ncT5X+7VTU1Ojb37zm1q1apVWrVqlW265RbNmzdLDDz/MtTNGPKwnAXp7e7V161YdPXpUTqdTd999\nN/uE6+IWRj/72c/U3t4ui8US+mDmtddem+qlpUx9fb0aGhpGvLZw4UItWrRIR44c0Y4dO9TV1aWy\nsjLdd999crvdKVpp8kXKpqSkRO+//756enrkcDh0zTXXaMmSJcrPz0/RSpPP6/XqRz/6kWw2W2jP\nfUlavny5qqurr/prxygfi8VyVV8/PT09ev3119XS0qJgMCiXy6XbbrsttG3s1X7tGOWzb9++q/ra\nuVx9fb3OnTsX2if8ar92xoISDgAAACQZ4ygAAABAklHCAQAAgCSjhAMAAABJRgkHAAAAkowSDgAA\nACQZJRwAAABIMko4AAAAkGSUcAAAACDJKOEAAABAklHCAQAAgCSjhAMAAABJRgkHAAAAkowSDgAA\nACQZJRwAAABIMko4AAAAkGSUcAAAACDJKOEAAABAklHCAcCEpk+frvfeey/09WuvvSa3262GhoYU\nrgoAEC1KOACY3KZNm7RmzRpt375dCxcuTPVyAABRoIQDgIn90z/9k7797W/rv/7rv3THHXekejkA\ngChZgsFgMNWLAADEZvr06aqpqdEHH3ygd955R3PmzEn1kgAAMeBOOACY1Lvvvqvbb79dN910U6qX\nAgCIESUcAEzqxRdf1B/+8Af91V/9lfhLTQAwF0o4AJjUpEmT9P777+t//ud/tHr16lQvBwAQA0o4\nAJjYlClT9P777+vtt9/W2rVrU70cAECUslK9AADA+FRUVKiurk533XWXJkyYoP/3//5fqpcEABgF\nu6MAAAAAScY4CgAAAJBklHAAAAAgySjhAAAAQJJRwgEAAIAko4QDAAAASUYJBwAAAJKMEg4AAAAk\nGSUcAAAASDJKOAAAAJBk/x/cpbu4c564DwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d4eab448d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ggplot: (-9223371910980871413)>"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=pd.DataFrame({'Scores':cv_scores,'K':myList})\n",
    "ggplot(a,aes('K','Scores'))+geom_point()+geom_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) These accuracy scores look very promising compared to, say, classifying the wine using a coinflip. However, in binary classification problems, accuracy can be misleading if one class (say, bad wine) is much more common than another (say, good wine), this is, when the classes are unbalanced.\n",
    "\n",
    "Print the percentage of wines that are labeled as \"bad\" in the dataset and plot the same boxplot as the last question (feel free to copy/paste), but this time draw a line across the plot denoting the accuracy of always guessing zero (\"bad wine\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.int32' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-291-347135e5f331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mggplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'K'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Scores'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mgeom_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mgeom_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mgeom_abline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mslope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.int32' has no len()"
     ]
    }
   ],
   "source": [
    "ggplot(a,aes('K','Scores'))+geom_point()+geom_line()+geom_abline(intercept=len(Y[Y==0])/len(Y),slope=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "When there are unbalanced classes in a dataset, guessing the more common class will often yield very high accuracy. For this reason, we usually want to use different metrics that are less sensitive to imbalance when evaluating the predictive performance of classifiers. These metrics were originally developed for clinical trials, so to keep with the standard terminology, we define \"good\" wines (value of 1) as \"positive\" and the \"bad\" wines (value of 0) as the \"negatives\". We then define the following:\n",
    "\n",
    "P - number of positives in the sample.\n",
    "\n",
    "N - number of negatives in the sample.\n",
    "\n",
    "TP - number of true positives: how many of the \"positive\" guesses of the classifier are true.\n",
    "\n",
    "FP - number of false positives: how many of the \"positive\" guesses of the classifier are actually negatives.\n",
    "\n",
    "TN - number of true negatives; similarly, this is how many of the \"negative\" guesses of the classifier are true.\n",
    "\n",
    "FN - number of false negatives; how many of the \"negative\" guesses are actually positives.\n",
    "\n",
    "When calling the score functions in scikit-learn you obtained the default measure of efficiency, which is called accuracy. This is simply the ratio of successful guesses (both positives and negatives) across all samples:\n",
    "accuracy=TP+TNP+N.\n",
    "accuracy=TP+TNP+N.\n",
    "In our case, when the two classes (good and bad wines) are very unbalanced in the sample, we should look for a better measure of efficiency.\n",
    "\n",
    "Usually, the goal is to identify the members of the positive class (the rare class) successfully -- this could be either the good wines or the patients presenting a rare disease. It is common practice to define the following ratios:\n",
    "\n",
    "The recall rate (also called the sensitivity or the true positive rate) is the ratio of true positive guesses among all positives:\n",
    "recall=TP=TP+FN.\n",
    "recall=TP=TP+FN.\n",
    "The precision is the ratio of the true positive guesses over all the positive guesses:\n",
    "precision=TP+FP.\n",
    "precision=TP+FP.\n",
    "(e) Describe in words what the difference is between precision and recall. Describe an application scenario where precision would be more important than recall, and one scenario where recall would be more important than precision.\n",
    "\n",
    "YOUR ANSWER HERE.\n",
    "\n",
    "Because precision and recall both provide valuable information about the quality of a classifier, we often want to combine them into a single general-purpose score. The F1 score is defined as the harmonic mean of recall and precision:\n",
    "F1=2×recall×precisionrecall+precision.\n",
    "F1=2×recall×precisionrecall+precision.\n",
    "The harmonic mean of two numbers is closer to the smaller of the two numbers than the standard arithmetic mean. The F1 score thus tends to favor classifiers that are strong in both precision and recall, rather than classifiers that emphasize one at the cost of the other.\n",
    "\n",
    "(f) For this part, repeat the cross-validation analysis in part (b) changing the scoring parameter of the cross_val_score function such that the measure used is the F1 score. Comment briefly on these numbers. Hint: See the scikit-learn documentation for the options you can use for the scoring parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myList = list(range(1,41))\n",
    "cv_scores = []\n",
    "for k in myList:\n",
    "    Rf = RandomForestClassifier(n_estimators=k)\n",
    "    scores = cross_val_score(Rf, X,Y, cv=10, scoring='f1')\n",
    "    cv_scores.append(scores.mean())\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=pd.DataFrame({'Scores':cv_scores,'K':myList})\n",
    "ggplot(a,aes('K','Scores'))+geom_point()+geom_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: \n",
    "Classifier Calibration\n",
    "Many classifiers, including random forest classifiers, can return prediction probabilities, which can be interpreted as the probability that a given prediction point falls into a given class (i.e., given the data X and a candidate class C, the prediction probability states P(Y=c|X)P(Y=c|X)). However, when the classes in the training data are unbalanced, as in this wine example, these prediction probabilities calculated by a classifier can be inaccurate. This is because many classifiers, again including random forests, do not have a way to internally adjust for this imbalance.\n",
    "\n",
    "Despite the inaccuracy caused by imbalance, the prediction probabilities returned by a classifier can still be used to construct good predictions if we can choose the right way to turn a prediction probability into a prediction about the class that the datapoint belongs to. We call this task calibration.\n",
    "\n",
    "If a classifier's prediction probabilities are accurate, the appropriate way to convert its probabilities into predictions is to simply choose the class with probability > 0.5. This is the default behavior of classifiers when we call their predict method. When the probabilities are inaccurate, this does not work well, but we can still get good predictions by choosing a more appropriate cutoff. In this question, we will choose a cutoff by cross validation.\n",
    "\n",
    "(a) Fit a random forest classifier to the wine data using 15 trees. Compute the predicted probabilities that the classifier assigned to each of the training examples (Hint: Use the predict_proba method of the classifier after fitting.). As a sanity test, construct a prediction based on these predicted probabilities that labels all wines with a predicted probability of being in class 1 > 0.5 with a 1 and 0 otherwise. For example, if originally probabilities =[0.1,0.4,0.5,0.6,0.7]=[0.1,0.4,0.5,0.6,0.7], the predictions should be [0,0,0,1,1][0,0,0,1,1]. Compare this to the output of the classifier's predict method, and show that they are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Rf = RandomForestClassifier(n_estimators=15)\n",
    "Rf.fit(X,Y)\n",
    "My_Predict=np.array([1 if y>.5 else 0 for y in Rf.predict_proba(X)[:,1]])\n",
    "My_Predict==list(Rf.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Write a function cutoff_predict that takes a trained classifier, a data matrix X, and a cutoff, and generates predictions based on the classifier's predicted probability and the cutoff value, as you did in the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cutoff_P(Rf,X,cutoff):\n",
    "    My_predict=[1 if y>cutoff else 0 for y in Rf.predict_proba(X)[:,1]]\n",
    "    return My_predict\n",
    "cutoff_P(Rf,X,.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Using 10-fold cross validation find a cutoff in np.arange(0.1,0.9,0.1) that gives the best average F1 score when converting prediction probabilities from a 15-tree random forest classifier into predictions.\n",
    "\n",
    "To help you with this task, we have provided you a function custom_f1 that takes a cutoff value and returns a function suitable for using as the scoring argument to cross_val_score. This function uses the cutoff_predict function that you defined in the previous question.\n",
    "\n",
    "Using a boxplot, compare the F1 scores that correspond to each candidate cutoff value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_f1(cutoff):\n",
    "    def f1_cutoff(Rf, X, Y):\n",
    "        ypred = cutoff_P(Rf,X,cutoff)\n",
    "        return sklearn.metrics.f1_score(Y, ypred)\n",
    "        \n",
    "    return f1_cutoff\n",
    "\n",
    "cv_scores=[]\n",
    "l=np.arange(0.1,0.9,0.1)\n",
    "for cutoff in l:\n",
    "    Rf = RandomForestClassifier(n_estimators = 15)\n",
    "    scores = cross_val_score(Rf, X, Y, cv=10, scoring=custom_f1(cutoff))   \n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "cv_scores\n",
    "a=pd.DataFrame({'Scores':cv_scores,'cutoff':l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ggplot(a,aes('cutoff','Scores'))+geom_point()+geom_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Visualizing Classifiers Using Decision Surfaces\n",
    "One common visual summary of a classifier is its decision surface. Recall that a trained classifier takes in features XX and tries to predict a target YY. We can visualize how the classifier translates different inputs X into a guess for Y by plotting the classifier's prediction probability (that is, for a given class cc, the assigned probability that Y=cY=c) as a function of the features XX. Most classifiers in scikit-learn have a method called predict_proba that computes this quantity for new examples after the classifier has been trained.\n",
    "\n",
    "(a) Decision surface visualizations are really only meaningful if they are plotted against inputs XX that are one- or two-dimensional. So before we plot these surfaces, we will first find two \"important\" dimensions of XX to focus on. Recall that in the last homework we used SVD to perform a similar task. Here, we will use a different dimension reduction method based on random forests.\n",
    "\n",
    "Random forests allow us to compute a heuristic for determining how \"important\" a feature is in predicting a target. This heuristic measures the change in prediction accuracy if we take a given feature and permute (scramble) it across the datapoints in the training set. The more the accuracy drops when the feature is permuted, the more \"important\" we can conclude the feature is. Importance can be a useful way to select a small number of features for visualization.\n",
    "\n",
    "As you did in the last question, train a random forest classifier on the wine data using 15 trees. Use the feature_importances_ attribute of the classifier to obtain the relative importance of the features. These features are the columns of the dataframe. Show a simple bar plot showing the relative importance of the named features of the wines in the databes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Rf = RandomForestClassifier(n_estimators=15)\n",
    "\n",
    "Rf.fit(X,Y)\n",
    "importance_list=Rf.feature_importances_\n",
    "importance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({'Importance':importance_list, 'Variables':Wine.columns})\n",
    "importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Below, we have provided you with a function plot_decision_surface that plots a classifier's decision surface, taking as arguments a classifier object, a two-column feature matrix, and a target vector.\n",
    "\n",
    "Using this function and the results from the \"importance\" analysis above, subset the data matrix to include just the two features of highest importance. Then plot the decision surfaces of a decision tree classifier, and a random forest classifier with number of trees set to 15, and a support vector machine with C set to 100, and gamma set to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sklearn.linear_model\n",
    "import sklearn.svm\n",
    "\n",
    "def plot_decision_surface(clf, X_train, Y_train):\n",
    "    plot_step=0.1\n",
    "    \n",
    "    if X_train.shape[1] != 2:\n",
    "        raise ValueError(\"X_train should have exactly 2 columnns!\")\n",
    "    \n",
    "    x_min, x_max = X_train[:, 0].min() - plot_step, X_train[:, 0].max() + plot_step\n",
    "    y_min, y_max = X_train[:, 1].min() - plot_step, X_train[:, 1].max() + plot_step\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    clf.fit(X_train,Y_train)\n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "    else:\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])    \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Reds)\n",
    "    plt.scatter(X_train[:,0],X_train[:,1],c=Y_train,cmap=plt.cm.Paired)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp_cols = Rf.feature_importances_.argsort()[::-1][0:2]\n",
    "X_imp = X[:,imp_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_decision_surface(DecisionTreeClassifier(), X_imp, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_decision_surface(RandomForestClassifier(n_estimators=15), X_imp, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_decision_surface(RandomForestClassifier(n_estimators=15), X_imp, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_decision_surface(sklearn.svm.SVC(C=100.0, gamma=1.0), X_imp, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) The SVM implementation of sklearn has an optional parameter class_weight. This parameter is set to None per default, but it also provides an auto mode, which uses the values of the labels Y to automatically adjust weights inversely proportional to class frequencies. As done in sub-problem 4(b), draw the decision boundaries for two SVM classifiers. Use C=1.0, and gamma=1.0 for both models, but for the first SVM set class_weigth to None, and for the second SVM set class_weigth to 'auto'. (Hint: None is a keyword in Python, whereas the 'auto' is a String and needs the quotation marks.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " plot_decision_surface(sklearn.svm.SVC(C=1.0, gamma=1.0, class_weight=None), X_imp, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_decision_surface(sklearn.svm.SVC(C=1.0, gamma=1.0, class_weight='balanced'),X_imp,Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
